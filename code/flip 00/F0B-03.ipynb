{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (00): Data Science \n",
    "**(Module 01: Data Science)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use,but NOT allowed to change and distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4 Supervised Learning\n",
    "\n",
    "\n",
    "The purpose of this session is to demonstrate different coefficient and linear regression.\n",
    "\n",
    "\n",
    "## Content\n",
    "\n",
    "### Part 1 Data Dependency\n",
    "\n",
    "1.1 [Pearson's-r Correlation coefficient](#pearson)\n",
    "\n",
    "1.2 [Spearman's rank coefficient](#spearman)\n",
    "\n",
    "\n",
    "### Part 2 Linear Regression\n",
    "\n",
    "2.1 [Multiple Linear Regression](#mlr)\n",
    "\n",
    "2.2 [Regression for Median House Price](#rmhp)\n",
    "\n",
    "### Part 3 Distances\n",
    "\n",
    "3.1 [Euclidean Distance](#euclidean)\n",
    "\n",
    "3.2 [Cosine Distance](#cosine)\n",
    "\n",
    "3.3 [Term-by-Document Matrix](#t2d)\n",
    "\n",
    "### Part 4 K-NN Classification\n",
    "\n",
    "4.1 [K-NN in Python](#knn)\n",
    "\n",
    "4.2 [Decision Boundary](#db)\n",
    "\n",
    "### Part 5 Naive Bayes Classifier\n",
    "\n",
    "5.1 [NBC by Example](#nbc)\n",
    "\n",
    "5.2 [NBC Exercise](#nbc2)\n",
    "\n",
    "### Part 6 Confidence Interval\n",
    "\n",
    "6.1 [Population and Sample](#popsample)\n",
    "\n",
    "6.2 [Confidence Interval](#ci)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">1. Data Dependency</span>\n",
    "\n",
    "<a id = \"pearson\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.1 Pearson's-r Correlation coefficient</span>\n",
    "\n",
    "\n",
    "We assume $X=\\left\\{ X_{1},\\ldots,X_{n}\\right\\}$ \n",
    "and $Y=\\left\\{ Y_{1},\\ldots,Y_{n}\\right\\}$. Then Pearson-r correlation coefficient is defined as \n",
    "\n",
    "$$ \\rho(X,Y) = \\frac{\\text{cov}(X,Y)}{\\sigma_X \\sigma_Y} =  \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n(X_i-\\bar{X})^2} \\sqrt{\\sum_{i=1}^n(Y_i-\\bar{Y})^2}} $$\n",
    "\n",
    "Use the car data and find the Pearson's-r correlation coefficient between car weights and fuel consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Auto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "miles = data['miles']\n",
    "weights = data['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print miles[:10]\n",
    "print weights[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_r = np.cov(miles, weights)[0, 1] / (miles.std() * weights.std())\n",
    "print pearson_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(miles,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "horse = data['Horse power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(weights,horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(7, 5), dpi=300)\n",
    "ax.scatter(weights,miles, alpha=0.6, edgecolor='none', s=100)\n",
    "ax.set_xlabel('Car Weight (tons)')\n",
    "ax.set_ylabel('Miles Per Gallon')\n",
    "\n",
    "line_coef = np.polyfit(weights, miles, 1)\n",
    "xx = np.arange(1, 5, 0.1)\n",
    "yy = line_coef[0]*xx + line_coef[1]\n",
    "\n",
    "ax.plot(xx, yy, 'r', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: \n",
    "\n",
    "1. Find the Pearson's-r coefficient for two linearly dependent variables. Add some noise and see the effect of varying the noise. \n",
    "2. Simulate and visualize some data with positive linear correlation\n",
    "3. Simulate and visualize some data with negative linear correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(-5, 5, 0.1)\n",
    "pp = 1.5  # level of noise\n",
    "yy = xx + np.random.normal(0, pp, size=len(xx))\n",
    "\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c='r', edgecolor='none')\n",
    "ax.set_xlabel('X data')\n",
    "ax.set_ylabel('Y data')\n",
    "\n",
    "line_coef = np.polyfit(xx, yy, 1)\n",
    "line_xx = np.arange(-5, 5, 0.1)\n",
    "line_yy = line_coef[0]*line_xx + line_coef[1]\n",
    "\n",
    "ax.plot(line_xx, line_yy, 'b', lw=2)\n",
    "\n",
    "print scipy.stats.pearsonr(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's r coefficient is limited to analyze the linear correlation between two variables. It is not capable to show the non-linear dependency. Investigate the Pearson's r coefficient between two variables that are correlated non-linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data, first for X\n",
    "xx = np.arange(-5, 5, 0.1)\n",
    "\n",
    "# assume Y = 2Y + some perturbation\n",
    "pp = 1.1  # level of noise\n",
    "yy = xx**2 + np.random.normal(0, pp, size=len(xx))\n",
    "\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c='r', edgecolor='b')\n",
    "ax.set_xlabel('X data')\n",
    "ax.set_ylabel('Y data')\n",
    "ax.set_title('$Y = X^2+\\epsilon$', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data, first for X\n",
    "xx = np.arange(-5, 5, 0.1)\n",
    "\n",
    "# assume Y = 2Y + some perturbation\n",
    "pp = 1.1  # level of noise\n",
    "yy = xx**2 + np.random.normal(0, pp, size=len(xx))\n",
    "\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xx, yy, c='r', edgecolor='b')\n",
    "ax.set_xlabel('X data')\n",
    "ax.set_ylabel('Y data')\n",
    "ax.set_title('$Y = X^2+\\epsilon$', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson's-r correlation is near zero which means there is no linear correlation. But how about non-linear correlation? Isn't $y=x^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"spearman\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.2 Spearman's rank coefficient</span>\n",
    "\n",
    "Spearman's rank coefficient is used for discrete/ordinal data. Find the Spearman's rank between horse power and number of cylinders of the car data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#horse = np.array([float(dd[4]) for dd in data[1:]])\n",
    "#cylinder = np.array([float(dd[2]) for dd in data[1:]])\n",
    "horse = data['Horse power']\n",
    "cylinder = data['cylinder number']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5), dpi=300)\n",
    "ax.scatter(horse, cylinder, alpha=0.6, edgecolor='none', s=100)\n",
    "ax.set_xlabel('Horse power')\n",
    "ax.set_ylabel('#Cylinders')\n",
    "\n",
    "print scipy.stats.spearmanr(horse, cylinder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**. \n",
    "Compute the spearman rank correlation between \"Horse power\" and \"Engine displacement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement = data['Engine displacement']\n",
    "scipy.stats.spearmanr(horse,displacement)\n",
    "\n",
    "radius = 150*np.ones(len(horse))\n",
    "fig, ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "ax.scatter(displacement, horse, alpha=0.2, c='m', edgecolor='none',s=radius)\n",
    "\n",
    "ax.set_xlabel('Engine displacement')\n",
    "ax.set_ylabel('Horse power')\n",
    "plt.savefig(\"Engine_vs_horse.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">2. Linear Regression</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we investigate a simple case by fitting a linear regression for three data points. First we simulate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating the data\n",
    "\n",
    "x = np.c_[0, 1, 2, 1.5].T\n",
    "y  = [1, 1.5, 3.1, 1.5]\n",
    "\n",
    "print x\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the data\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\n",
    "ax.scatter(x, y, c='r')\n",
    "ax.set_title('simulated data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# instanciate the model\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Coefficients:\", lr.coef_\n",
    "print \"   Intercept:\", lr.intercept_\n",
    "print \"    Residues:\", lr.residues_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the line to see how it estimates our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lr.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=300)\n",
    "ax.scatter(x, y, c='r')\n",
    "ax.plot(x, yhat)\n",
    "\n",
    "ax.set_title('simulated data and the estimated line')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the method `predict()` to predict `y` for a new `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.c_[4, 2.3].T\n",
    "y_test = lr.predict(x_test)\n",
    "\n",
    "print x_test.T\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"mlr\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">2.1 Multiple Linear Regression</span>\n",
    "\n",
    "\n",
    "Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data. Every value of the independent variable x is associated with a value of the dependent variable y. For example if we have two explanatory variables (attributes, features), our data has such a form:\n",
    "\n",
    "$$\n",
    "D=\\left\\{ \\left(\\left(x_{1,1},x_{2,1}\\right),y_{1}\\right),\\left(\\left(x_{1,2},x_{2,2}\\right),y_{2}\\right),\\ldots,\\left(\\left(x_{1,n},x_{2,n}\\right),y_{n}\\right)\\right\\} \n",
    "$$\n",
    "\n",
    "Now we fit a multiple linear regression $y = x_1 + 2x_2 + 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the data\n",
    "\n",
    "x = np.c_[[0, 0], [0, 1], [1, 1], [1, 0]].T\n",
    "y = [1.5, 3.2, 4, 2]\n",
    "\n",
    "print x\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = linear_model.LinearRegression(fit_intercept=True)\n",
    "mlr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mlr.coef_\n",
    "print mlr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mlr.residues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mlr.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises 3**: \n",
    "\n",
    "As the score suggests, now we have the perfect regression. Change the values of $y$ slightly and see what effect it has on the `mlr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"rmhp\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">2.2 Regression for median house prices</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the package `pandas` for reading and storing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/housing_300.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the scatter plot of the number of rooms vs the median house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7), dpi=300)\n",
    "median_prices = data['MEDV']\n",
    "avg_rooms = data['RM']\n",
    "scales = 50*np.ones(len(median_prices))\n",
    "ax.scatter(avg_rooms, median_prices, color='b',s=scales, alpha=0.7, edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print avg_rooms.shape\n",
    "print median_prices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How correlated are the number of rooms and the price of the house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(avg_rooms, median_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to fit a linear regression mode on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "\n",
    "x = np.c_[avg_rooms.values]\n",
    "y = median_prices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print lr.coef_\n",
    "print lr.intercept_\n",
    "print lr.residues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the model parameters\n",
    "\n",
    "print lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "\n",
    "yhat = lr.predict(x)\n",
    "print x[:10]\n",
    "print yhat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the result\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "\n",
    "scales = 20*np.ones(len(median_prices))\n",
    "ax.scatter(avg_rooms,median_prices,color='b',s=scales,alpha=0.7,edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')\n",
    "\n",
    "# plot the regression linear leared\n",
    "ax.plot(x,yhat)\n",
    "\n",
    "# visualize the residuals\n",
    "tmp = np.reshape(x,[1,len(x)])[0]\n",
    "tmp_x = []\n",
    "tmp_y = []\n",
    "for i in xrange(len(x)):\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,y[i])\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,yhat[i])\n",
    "    ax.plot(tmp_x,tmp_y,color='g',linewidth=0.5)\n",
    "    tmp_x = []\n",
    "    tmp_y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out the sum of residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.residues_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is customary to test your model on **unseen** data. So we divide our data into two parts. We use 70% of it to train the model and 30% to evaluate its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.7\n",
    "split_idx = int(np.round(split * len(data)))\n",
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[0:200]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "train_data.plot(kind='scatter', x='RM', y='MEDV', ax=axs[0], figsize=(7, 7))\n",
    "train_data.plot(kind='scatter', x='AGE', y='MEDV', ax=axs[1], figsize=(7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[200:300]\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data['RM'].values\n",
    "train_X = np.c_[train_X]\n",
    "train_Y = train_data['MEDV'].tolist()\n",
    "\n",
    "test_X = test_data['RM'].values\n",
    "test_X = np.c_[test_X]\n",
    "test_Y = test_data['MEDV'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print type(train_X)\n",
    "print train_X.shape\n",
    "print type(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build a linear regression model from training data\n",
    "'''\n",
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "lr.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print lr.coef_\n",
    "print lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the linear regression result and the data to see how it fits the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "\n",
    "# plot training data\n",
    "scales = 20*np.ones(len(train_Y))\n",
    "ax.scatter(train_X,train_Y,color='b',s=scales,alpha=0.7,edgecolor='r')\n",
    "plt.xlabel('$X$ (number of rooms)')\n",
    "plt.ylabel('$Y$ (median house prices)')\n",
    "plt.title('Training a simple linear regression model')\n",
    "\n",
    "# plot the regression line\n",
    "train_Yhat = lr.predict(train_X)\n",
    "plt.plot(train_X,train_Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the model parameters, we can use the model to predict for unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_test = lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,7),dpi=300)\n",
    "\n",
    "# plot the predicted points along the prediction line\n",
    "scales = 30*np.ones(len(test_X))\n",
    "ax.scatter(test_X,yhat_test,s=scales,color='b',edgecolor='r')\n",
    "ax.plot(test_X,yhat_test,color='b',linewidth=.2)\n",
    "\n",
    "# plot the true values\n",
    "scales = 30*np.ones(len(test_X))\n",
    "ax.scatter(test_X,test_Y,s=scales,color='g',edgecolor='b')\n",
    "\n",
    "# plot the residual line\n",
    "tmp = np.reshape(test_X,[1,len(test_X)])[0]\n",
    "tmp_x = []\n",
    "tmp_y = []\n",
    "for i in xrange(len(test_X)):\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,yhat_test[i])\n",
    "    tmp_x = np.append(tmp_x,tmp[i])\n",
    "    tmp_y = np.append(tmp_y,test_Y[i])\n",
    "    ax.plot(tmp_x,tmp_y,color='red',linewidth=0.5)\n",
    "    tmp_x = []\n",
    "    tmp_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">3. Distances</span>\n",
    "\n",
    "`Distance` is a numerical description of how far apart objects are. It is a concrete way of describing what it means for elements of some space to be close or far away from each other, for example the distance between two vectors in an 2-dimensional space.\n",
    "\n",
    "Now that you have know how to represent an n-dimensional vector in Python with NumPy arrays, we will write a function as a metric to measure the distance between two vectors. There are multiple ways to measure the distance between two vectors. We will discuss Euclidean distance and cosine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id = \"euclidean\"></a>\n",
    "\n",
    "\n",
    "### <span style=\"color:#0b486b\">3.1 Euclidean Distance</span>\n",
    "\n",
    "Euclidean distance comes from Geometry. If we assume $\\mathbf{x}_{1}=\\left[x_{11},x_{12},\\ldots,x_{1n}\\right]$ and $\\mathbf{x}_{2}=\\left[x_{21},x_{22},\\ldots,x_{2n}\\right]$, then the Euclidean distance between $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ is defined as:\n",
    "\n",
    "$$d\\left(\\mathbf{x}_{1},\\mathbf{x}_{2}\\right)=\\sqrt{\\left(x_{11}-x_{21}\\right)^{2}+\\left(x_{12}-x_{22}\\right)^{2}+\\ldots+\\left(x_{1n}-x_{2n}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "We can use array operators for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([2, 5, 4, 6, 8])\n",
    "x2 = np.array([3, 5, 6, 8, 6])\n",
    "\n",
    "print x1 - x2\n",
    "print (x1 - x2) ** 2\n",
    "print np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance1(x1, x2):\n",
    "    d = x1 - x2\n",
    "    d = d ** 2\n",
    "    return np.sqrt(d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([-1, 2, 0, 5])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "\n",
    "print euclidean_distance1(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since two vectors passed to the function should be the same size, it is better to perform a sanity check before applying the subtraction. Otherwise it will raise an error. We can do this by using `if - elif` statement or as a better practice by using `try - except`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def euclidean_distance2(x1, x2):\n",
    "    if x1.shape[0] != x2.shape[0]:\n",
    "        sys.exit('x1 and x2 are not the same size')\n",
    "    else:\n",
    "        d = x1 - x2\n",
    "        d = d ** 2\n",
    "        return np.sqrt(d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this cell\n",
    "\n",
    "x1 = np.array([-1, 2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "euclidean_distance2(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance3(x1, x2):\n",
    "    try:\n",
    "        d = x1 - x2\n",
    "        d = np.power(d, 2)\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print \"Vectors passed to the function are not the same size\"\n",
    "        # you can return a default value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this cell\n",
    "\n",
    "x1 = np.array([-1, 2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 2])\n",
    "a = euclidean_distance3(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance4(x1, x2):\n",
    "    try:\n",
    "        d = np.array(x1) - np.array(x2)\n",
    "        d = np.power(d, 2)\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print \"Vectors passed to the function are not the same size\"\n",
    "        # you can return a default value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"cosine\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">3.2 cosine similarity and distance</span>\n",
    "\n",
    "Cosine similarity is a measure of similarity between two vectors based on the angle between them. Cosine similarity is widely used in information retrieval and text mining as a measure of similarity between documents and is defined as:\n",
    "\n",
    "$$S_{c}\\left(\\mathbf{x}_{1},\\mathbf{x_{2}}\\right)=\\frac{\\mathbf{x}_{1}.\\mathbf{x_{2}}}{\\parallel\\mathbf{x}_{1}\\parallel^{2}+\\parallel\\mathbf{x}_{2}\\parallel^{2}-\\mathbf{x}_{1}.\\mathbf{x_{2}}}$$\n",
    "\n",
    "\n",
    "Cosine similarity is particularly used in positive space where the outcome is bounded in [0, 1]. The cosine distance is defined as the complement to cosine similarity in positive space that is $D_{c}\\left(x_{1},x_{2}\\right)=1-S_{c}\\left(x_1,x_2\\right)$ where $D_c$ is the cosine distance and $S_c$ is the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([1,2,3])\n",
    "x2 = np.array([3,4,6])\n",
    "\n",
    "x1 * x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_distance(x1, x2):\n",
    "    try:\n",
    "        num = (x1*x2).sum()\n",
    "        denom = (x1*x1).sum() + (x2*x2).sum() - (x1*x2).sum()\n",
    "        num += 0.0    # or use np.astype(float) to make sure of float division\n",
    "        return 1 - num/denom\n",
    "    except ValueError as e:\n",
    "        print \"Vectors passed to the function are not the same size\"\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "cosine_distance(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"t2d\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">3.3 Term-by-Document matrix</span>\n",
    "\n",
    "A term-by-document matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occur in the document collection. Each row corresponds to a document and each column correspond to a term. Thus the value that appears in row $j$ and column $i$ represents the frequency of appearing term $i$ in document $j$.\n",
    "\n",
    "We will represent two datasets with term-by-document matrix:\n",
    "\n",
    "* a collection of 100 Twitter messages about Geelong\n",
    "* a collection of 6 news articles (5 about Apple and 1 about politics)\n",
    "\n",
    "The data is already collected and stores in text files. Thus you will need to:\n",
    "\n",
    "* read the text files\n",
    "    * using file object\n",
    "* perform pre-processing\n",
    "    * using string methods\n",
    "    * using re package\n",
    "* construct the term-by-document matrix\n",
    "    * using numpy arrays and operations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Twitter dataset\n",
    "First read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get current working directory\n",
    "cwd = os.getcwd()   \n",
    "\n",
    "# join the subdirectory of the data and data file name\n",
    "file_path = os.path.join(cwd, \"data/tweets.txt\")\n",
    "\n",
    "# read the contents of the file and store it in a list\n",
    "with open(file_path) as fp:\n",
    "    tweets = fp.readlines()    \n",
    "for tweet in tweets:\n",
    "    print tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly when dealing with data, we have to perform some sort of data pre-processing. Data collection is often loosely controlled, resulting in out of the range values, missing values, and etc. Thus quality of the data is first and formost before running an analysis. This step is specific to the nature of the data. For example for text data it may consist of cleaning, normalization, tokenization, and etc. \n",
    "\n",
    "In this case, our pre-processing consists of:\n",
    "\n",
    "* converting all the words into lower case to remove the effect of the letter case\n",
    "* replacing the URLs with a simple string such as 'url'. From the previous cell, you should be able to see that many of the tweets contain a URL. Since we are not using them now, we can remove them or replace them.\n",
    "* Removing the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def pre_process(doc):\n",
    "    \"\"\"\n",
    "    pre-processes a doc\n",
    "      * Converts the tweet into lower case,\n",
    "      * removes the URLs,\n",
    "      * removes the punctuations\n",
    "      * tokenizes the tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    # gettign rid of non ascii codes\n",
    "    doc = doc.decode('ascii', 'ignore')\n",
    "    \n",
    "    # repalcing URLs\n",
    "    url_pattern = \"http://[^\\s]+|https://[^\\s]+|www.[^\\s]+|[^\\s]+\\.com|bit.ly/[^\\s]+\"\n",
    "    doc = re.sub(url_pattern, 'url', doc) \n",
    "\n",
    "    punctuation = r\"\\(|\\)|#|\\'|\\\"|-|:|\\\\|\\/|!|_|,|=|;|>|<|\\.\"\n",
    "    doc = re.sub(punctuation, ' ', doc)\n",
    "    \n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print r['text']\n",
    "pre_process(r['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Use the function provided to pre-process one of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termdoc(docs):\n",
    "    \"\"\"\n",
    "    returns the term-by-document matrix and the vocabulary of the passed corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()   \n",
    "    termdoc_sparse = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # pre-process the doc\n",
    "        doc_tokens = pre_process(doc)\n",
    "        # computes the frequencies for doc\n",
    "        doc_sparse = Counter(doc_tokens)    \n",
    "\n",
    "        termdoc_sparse.append(doc_sparse)\n",
    "\n",
    "        # update the vocab\n",
    "        vocab.update(doc_sparse.iterkeys())  \n",
    "\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    n_vocab = len(vocab)\n",
    "    termdoc_dense = np.zeros((n_docs, n_vocab), dtype=int)\n",
    "\n",
    "    for j, doc_sparse in enumerate(termdoc_sparse):\n",
    "        for term, freq in doc_sparse.iteritems():\n",
    "            termdoc_dense[j, vocab.index(term)] = freq\n",
    "            \n",
    "    return termdoc_dense, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_termdoc, tweets_vocab = termdoc(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets look at one of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print tweets[j]\n",
    "print tweets_termdoc[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_vocab.index('beyond')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_termdoc[j][127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So baiscally, now each tweet is represented by a vector of size `len(tweets_vocab)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 News dataset\n",
    "Similar to previous sections, the data is stored in text files names as'news1.txt', ..., 'news5.txt'. All we have to do is read the files, construct the corpus and send it to `termdoc()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = 6\n",
    "cwd = os.getcwd()   \n",
    "news = []\n",
    "\n",
    "for j in xrange(1, n_docs+1):\n",
    "    filename = \"news{}.txt\".format(j)\n",
    "    file_path = os.path.join(cwd, \"data/{}\".format(filename))\n",
    "    with open(file_path) as fp:\n",
    "        news.append(fp.read())\n",
    "\n",
    "news_termdoc, news_vocab = termdoc(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now each news article is represented with a large vector of size `len(news_vocab)`. We can do many things with this representation. For example measuring the distance between two documents. The first 5 news articles are tech news and about Apple, but the 6th one is about politics. We expect that tech news be more similar to each other rather than to the politics news. In other words the distance between two articles from tech news should be less than the distance between a tech news article and a news article about politics. This is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cosine_distance(news_termdoc[1], news_termdoc[2])   # both aout Apple\n",
    "print cosine_distance(news_termdoc[1], news_termdoc[4])   # one from tech world, the other from politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\">4. k-Nearest Neighbours Classification</span> \n",
    "\n",
    "kNN is a non-parametric classification technique which is extensively used in practice. Its input consists of the `k` closest training examples and the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its `k` nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "**Please note that kNN is different from K-means.** K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster be close to each other. It is unsupervised because the points have no external classification. kNN is a classification algorithm that in order to determine the classification of a point, combines the class of the k nearest points. It is supervised because you are trying to classify a point based on the known label of other points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"knn\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">4.1 kNN in Python</span> \n",
    "\n",
    "To be able to illustrate how we perform kNN classification in Python, we need some data first. Therefore we synthesize some data from 3 classes. We assume the data in each class comes from a multivariate random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "n_per_class = 50\n",
    "colors = ['green', 'blue', 'magenta']\n",
    "\n",
    "mean1 = [-5, 10]\n",
    "cov1 = [[1.5, 0], [0, 1.5]]\n",
    "mean2 = [0, 7]\n",
    "cov2 = [[1.5, 0], [0, 3]]\n",
    "mean3 = [-6, 6]\n",
    "cov3 = [[2, 0], [0, 1.5]]\n",
    "\n",
    "means = [mean1, mean2, mean3]\n",
    "covs = [cov1, cov2, cov3]\n",
    "\n",
    "x11, x12 = np.random.multivariate_normal(means[0], covs[0], n_per_class).T\n",
    "x21, x22 = np.random.multivariate_normal(means[1], covs[1], n_per_class).T\n",
    "x31, x32 = np.random.multivariate_normal(means[2], covs[2], n_per_class).T\n",
    "\n",
    "scale = 75\n",
    "alpha = 0.6\n",
    "\n",
    "fig, ax  = plt.subplots(figsize=(7, 7), dpi=300)\n",
    "ax.scatter(x11, x12, alpha=alpha, color=colors[0], s=scale)\n",
    "ax.scatter(x21, x22, alpha=alpha, color=colors[1], s=scale)\n",
    "ax.scatter(x31, x32, alpha=alpha, color=colors[2], s=scale)\n",
    "\n",
    "ax.set_title(\"synthesized data for 3 classes\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to instantiate a kNN classifier from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "weights='uniform'\n",
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k,weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass one array as training features and on array as training labels to the `knn` object. Therefore we have to put all the attributes together (also class labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.r_[x11, x21, x31]\n",
    "x2 = np.r_[x12, x22, x32]\n",
    "X_train = np.c_[x1, x2]\n",
    "\n",
    "print x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.r_[0*np.ones(n_per_class), 1*np.ones(n_per_class), 2*np.ones(n_per_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how kNN classifies a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k)\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "X_test = [-7, 10]\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "ax.scatter(X_test[0], X_test[1], marker=\"x\", s=scale, lw=2, c='k')\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"db\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">4.2 Decision Boundry</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN effectively partitions the feature space into different sets and assigns the same class label to points belonging to the same partition. This partitioning changes as we change k. We illustrate this below. As you see bigger values of k, partition the space more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.05\n",
    "\n",
    "# Create colour maps\n",
    "cmap_light = ListedColormap(['#AAFFAA', '#AAAAFF', '#FFAAAA'])\n",
    "cmap_bold = ListedColormap(['green', 'blue', 'magenta'])\n",
    "\n",
    "x1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will investigate the effect of `'k'` on decision boundaries. Lets train a classifier with `k=1` which means we only use the label of the closest point to predict the label of a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Prediction\n",
    "\n",
    "Play with the `X_test` and `k` to see how the classifier behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn = neighbors.KNeighborsClassifier(k, weights=weights)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "Z = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7, 7))\n",
    "ax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n",
    "\n",
    "plt.xlim(xx1.min(), xx1.max())\n",
    "plt.ylim(xx2.min(), xx2.max())\n",
    "plt.title(\"3-Class classification (k = {})\".format(k))\n",
    "\n",
    "X_test = [-4, 8]\n",
    "Y_pred = knn.predict(X_test)\n",
    "ax.scatter(X_test[0], X_test[1], alpha=0.95, color='r', s=3*scale)\n",
    "\n",
    "ax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of predicting the class label for one point, we use our model to predict the labels of multiple points.\n",
    "\n",
    "First we generate some test data from the first class. This way we know the true class labels. Then we can use the `kNN` classifier to predict labels for the test data and get the predicted class labels. A measure of  accuracy for the classifier can be defined by comparing the true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 100\n",
    "X1_test, X2_test = np.random.multivariate_normal(mean1, cov1, n_test).T\n",
    "Y_true = 0 * np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.c_[X1_test, X2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times the classifier predicts the labels correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred == Y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(Y_pred == Y_true) + 0.0) / n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Repeat the previous experiment with a classifier which has been trained with a different `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use kNN to implement a classifier on handwritten digits dataset introduced in prac7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = digits['data']\n",
    "Y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred == Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(Y_pred == Y_test) + 0.0) / len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = Y_pred != Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 1))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_test[mask][i, :].reshape(8, 8))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"{}\".format(Y_pred[mask][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">5. Naive Bayes Classifier</span> \n",
    "\n",
    "\n",
    "Naive Bayes is one of the most practical classification machine learning algorithms. \n",
    "\n",
    "* fast\n",
    "* good performance\n",
    "* simple yet very effective\n",
    "* robust to irrelative features\n",
    "\n",
    "So why is it called naive?\n",
    "\n",
    "Because it does not consider the dependency between features and assume all features are independent of each other which is not the case in reality. This is a naive assumption, hence the name.\n",
    "\n",
    "The accuracy is very good although this naive assumption. A famous example of NB usage is spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"nbc\"></a>\n",
    "### <span style=\"color:#0b486b\">5.1 NBC by Example</span> \n",
    "\n",
    "We assume we have collected the below data for the past 5 days. Based on this data, can we predict if our subject will play in a setting like:\n",
    "\n",
    "    outlook  = overcast\n",
    "    temp     = hot\n",
    "    humidity = normal\n",
    "    windy    = no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"nb_data.png\" width=\"800\"> -->\n",
    "<img src=\"image/nb_data.png\" width=\"800\">\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to find a representation for our data. We can construct a dictionary to convert stings into numbers and then save them in a dataframe. \n",
    "\n",
    "    outlook: sunny=0, overcast=1, rainy=2\n",
    "    temp: hot=0, mild=1, cool=2\n",
    "    humidity: normal=0, high=1\n",
    "    wind: no=0, yes=1\n",
    "    play: np=0, yes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'outlook': [0, 1, 2, 0, 1],\n",
    "    'temp'   : [0, 1, 2, 1, 0],\n",
    "    'humid'  : [0, 0, 1, 0, 1],\n",
    "    'wind'   : [0, 0, 1, 1, 0],\n",
    "    'play'   : [1, 1, 0, 0, 0,]    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use Bayes rule to construct a Naive Bayes classifier. We can write:\n",
    "\n",
    "$$Pr\\left(p|o,t,h,w\\right)\\propto Pr\\left(p\\right)Pr(o|p)Pr(t|p)Pr(h|p)Pr(w|p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $Pr(p)$ we use marginal probablity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def marginal_prob(df, col):\n",
    "    ll = [(ss, (df[col] == ss).sum()) for ss in set(df[col])]\n",
    "    total_count = [b for a,b in ll]\n",
    "    total_count = sum(total_count)\n",
    "    \n",
    "    ll2 = [(a, b/total_count) for a, b in ll]\n",
    "    return dict(ll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate probability of a feature given the class (play) we use conditinoal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conditional_prob(df, f, c, val):\n",
    "    df2 = df[df[c] == val][f]\n",
    "    ll = [[ss, (df2 == ss).sum()] for ss in set(df2)]\n",
    "    total_count = [b for a,b in ll]\n",
    "    total_count = sum(total_count)\n",
    "    \n",
    "    ll2 = [(a, b/total_count) for a, b in ll]\n",
    "    return dict(ll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Bayes rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 1\n",
    "t = 0\n",
    "h = 0\n",
    "w = 0\n",
    "\n",
    "c = 0\n",
    "p0 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "c = 1\n",
    "p1 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "# normalizing\n",
    "p_sum = p0 + p1\n",
    "p0 /= p_sum\n",
    "p1 /= p_sum\n",
    "\n",
    "print \"probability of not playing: {}\".format(p0)\n",
    "print \"probability of playing    : {}\".format(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"nbc2\"></a>\n",
    "### <span style=\"color:#0b486b\">5.2 NBC Exercise</span> \n",
    "\n",
    "\n",
    "\n",
    "Suppose we have documents below as our training set. \n",
    "\n",
    "    d1: Chinese Beijing Chinese , class = C\n",
    "    d2: Chinese Chinese Shanghai, class = C\n",
    "    d3: Chinese Macao           , class = C\n",
    "    d4: Tokyo Japan Chinese     , class = J\n",
    "\n",
    "\n",
    "Train a NB classifier and predict if `d5` belongs to class C or J.\n",
    "\n",
    "    d5: Chinese Chinese Chinese Tokyo Japan, class = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">6. Confidence Interval</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"popsample\"></a>\n",
    "\n",
    "### <span style=\"color:#0b486b\">6.1 Population vs Sample</span> \n",
    "\n",
    "The main difference between population and sample comes down to how observations are assigned to dataset. A **population** includes all of the elements from the dataset. A **sample** consists of one or more observations from the population. In other words **population** is the entire collection of the desired measurable characteristic that we would have, if we could collect it. \n",
    "\n",
    "For example if suppose we want to find the average height of 2nd grade students in Australia. The population would be all the students who are studying in 2nd grade in Australia. But most probably we can not measure the height of all Australian 2nd grade students. It is not feasible. So what do we do? **We sample!**. We collect the height of some of Australian 2nd graders and based on that, we **estimate** the average height of the population (all of Australian 2nd grade students). The sample could be 2nd grade students of one class in one school, or multiple classes in multiple schools in one state, or  multiple schools in multiple states, or etc.\n",
    "\n",
    "A measurable statistic of a population (such as mean) is called a **parameter**. But a measurable characteristic of a sample is called **statistic**.\n",
    "\n",
    "----\n",
    "<a id = \"ci\"></a> \n",
    "\n",
    "### <span style=\"color:#0b486b\">6.2 Confidence Interval</span> \n",
    "\n",
    "As stated in previous section, population parameter is unknown. Confidence interval is a type of interval estimate of a population parameter calculated from sample statistics. It is an interval estimate combined with a probability.\n",
    "\n",
    "For the aforementioned example, it means that without collecting the height of all Aussie 2nd graders, we can estimate the average height by collecting a sample and using the below formula:\n",
    "\n",
    "\n",
    "$$Confidence\\, Interval=\\bar{X}\\pm z\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "$s$ is the sample standard deviation, $n$ is the sample size, and $z$ is often read from a table.\n",
    "\n",
    "    confidence level (%)     z\n",
    "         \n",
    "            70              1.04 \n",
    "            75              1.15\n",
    "            80              1.28 \n",
    "            85              1.44 \n",
    "            90              1.645\n",
    "            92              1.75\n",
    "            95              1.96\n",
    "            96              2.05\n",
    "            98              2.33\n",
    "            99              2.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use an example to clarify this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a population. Please note that we do not use the population in our computations. We use samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 10, 2\n",
    "n_population = 10000\n",
    "population = np.random.normal(mu, sigma, n_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(population, bins=25)\n",
    "ax.set_title(r\"population histogram, $\\mu={}$, $\\sigma={}$ \".format(mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample multiple times from this population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "sample_size = 500\n",
    "samples = np.zeros([n_trials, sample_size])\n",
    "sample_std = np.zeros(n_trials)\n",
    "sample_mean = np.zeros(n_trials)\n",
    "\n",
    "for i in range(n_trials):\n",
    "    samples[i] = np.random.choice(population, size=sample_size, replace=False)\n",
    "    sample_mean[i] = samples[i].mean()\n",
    "    sample_std[i] = samples[i].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 200\n",
    "se = sample_std[i] / np.sqrt(sample_size)\n",
    "dist = ss.distributions.norm(sample_mean[i], se)\n",
    "z = 1.96\n",
    "\n",
    "x = np.linspace(9.5, 10.5, 100)\n",
    "y = dist.pdf(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "    \n",
    "ax.hist(sample_mean, normed=True, bins=20, label='sample means')\n",
    "ax.plot(x, y, label=\"$N(\\mu_{i},se)$\")\n",
    "ax.vlines(sample_mean[i] + z*se, 0, 5, label='$\\mu_i \\pm z\\sigma_i$')\n",
    "ax.vlines(sample_mean[i] - z*se, 0, 5)\n",
    "\n",
    "ax.set_title(\"distribution of sample statistics\\ni={}\".format(i))\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
