{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP(01):  Advanced Data Science\n",
    "**(Tools Module 04: TPOP)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Session 06 - TPOT API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "class tpot.TPOTClassifier(generations=100, population_size=100,\n",
    "                          offspring_size=None, mutation_rate=0.9,\n",
    "                          crossover_rate=0.1,\n",
    "                          scoring='accuracy', cv=5,\n",
    "                          subsample=1.0, n_jobs=1,\n",
    "                          max_time_mins=None, max_eval_time_mins=5,\n",
    "                          random_state=None, config_dict=None,\n",
    "                          warm_start=False,\n",
    "                          memory=None,\n",
    "                          periodic_checkpoint_folder=None,\n",
    "                          early_stop=None,\n",
    "                          verbosity=0,\n",
    "                          disable_update_check=False)\n",
    "\n",
    "[source](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py)\n",
    "\n",
    "Automated machine learning for supervised classification tasks.\n",
    "\n",
    "The TPOTClassifier performs an intelligent search over machine learning pipelines that can contain supervised classification models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the [`scikit-learn API`](http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects). The TPOTClassifier will also search over the hyperparameters of all objects in the pipeline.\n",
    "\n",
    "By default, TPOTClassifier will search over a broad range of supervised classification algorithms, transformers, and their parameters. However, the algorithms, transformers, and hyperparameters that the TPOTClassifier searches over can be fully customized using the `config_dict` parameter.\n",
    "\n",
    "Read more in the [User Guide](https://epistasislab.github.io/tpot/using/#tpot-with-code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>Parameters:</td>\n",
    "        <td>**generations**: int, optional (default=100)<br>\n",
    "\n",
    "&emsp;Number of iterations to the run pipeline optimization process. Must be a positive number.\n",
    "<br><br>\n",
    "&emsp;Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline.\n",
    "<br><br>\n",
    "&emsp;TPOT will evaluate population_size + generations × offspring_size pipelines in total. \n",
    "<br><br>\n",
    "**population_size**: int, optional (default=100)\n",
    "<br>\n",
    "&emsp;Number of individuals to retain in the genetic programming population every generation. Must be a positive number.\n",
    "<br><br>\n",
    "&emsp;Generally, TPOT will work better when you give it more individuals with which to optimize the pipeline. \n",
    "<br><br>\n",
    "**offspring_size**: int, optional (default=100)\n",
    "<br>\n",
    "&emsp;Number of offspring to produce in each genetic programming generation. Must be a positive number. \n",
    "<br><br>\n",
    "**mutation_rate**: float, optional (default=0.9)\n",
    "&emsp;Mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation.\n",
    "<br><br>\n",
    "&emsp;mutation_rate + crossover_rate cannot exceed 1.0.\n",
    "<br><br>\n",
    "&emsp;We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms. \n",
    "<br><br>\n",
    "**crossover_rate**: float, optional (default=0.1)\n",
    "&emsp;Crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the genetic programming algorithm how many pipelines to \"breed\" every generation.\n",
    "<br><br>\n",
    "&emsp;mutation_rate + crossover_rate cannot exceed 1.0.\n",
    "<br><br>\n",
    "&emsp;We recommend using the default parameter unless you understand how the crossover rate affects GP algorithms. \n",
    "<br><br>\n",
    "**scoring**: string or callable, optional (default='accuracy')\n",
    "<br>\n",
    "&emsp;Function used to evaluate the quality of a given pipeline for the classification problem. The following built-in scoring functions can be used:\n",
    "<br><br>\n",
    "&emsp;'accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss','precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc'\n",
    "<br><br>\n",
    "&emsp;If you would like to use a custom scorer, you can pass the callable object/function with signature scorer(estimator, X, y).\n",
    "<br><br>\n",
    "&emsp;If you would like to use a metric function, you can pass the callable function to this parameter with the signature score_func(y_true, y_pred). TPOT assumes that any function with \"error\" or \"loss\" in the function name is meant to be minimized, whereas any other functions will be maximized. This scoring type was deprecated in version 0.9.1 and will be removed in version 0.11.\n",
    "<br><br>\n",
    "&emsp;See the section on scoring functions for more details. \n",
    "<br><br>\n",
    "**cv**: int, cross-validation generator, or an iterable, optional (default=5)\n",
    "<br>\n",
    "&emsp;Cross-validation strategy used when evaluating pipelines.\n",
    "<br><br>\n",
    "&emsp;Possible inputs:<br>\n",
    "\n",
    "&emsp;1. integer, to specify the number of folds in a StratifiedKFold,<br>\n",
    "&emsp;2. An object to be used as a cross-validation generator, or<br>\n",
    "&emsp;3. An iterable yielding train/test splits.<br>\n",
    "<br><br>\n",
    "**subsample**: float, optional (default=1.0)\n",
    "<br><br>\n",
    "&emsp;Fraction of training samples that are used during the TPOT optimization process. Must be in the range (0.0, 1.0].\n",
    "<br><br>\n",
    "&emsp;Setting subsample=0.5 tells TPOT to use a random subsample of half of the training data. This subsample will remain the same during the entire pipeline optimization process. \n",
    "<br><br>\n",
    "n_jobs: integer, optional (default=1)\n",
    "<br><br>\n",
    "&emsp;Number of processes to use in parallel for evaluating pipelines during the TPOT optimization process.\n",
    "<br><br>\n",
    "&emsp;Setting *n_jobs=-1* will use as many cores as available on the computer. Beware that using multiple processes on the same machine may cause memory issues for large datasets \n",
    "<br><br>\n",
    "**max_time_mins**: integer or None, optional (default=None)\n",
    "<br><br>\n",
    "&emsp;How many minutes TPOT has to optimize the pipeline.\n",
    "<br><br>\n",
    "&emsp;If not None, this setting will override the generations parameter and allow TPOT to run until max_time_mins minutes elapse. \n",
    "<br><br>\n",
    "**max_eval_time_mins**: integer, optional (default=5)\n",
    "<br><br>\n",
    "&emsp;How many minutes TPOT has to evaluate a single pipeline.\n",
    "<br><br>\n",
    "&emsp;Setting this parameter to higher values will allow TPOT to evaluate more complex pipelines, but will also allow TPOT to run longer. Use this parameter to help prevent TPOT from wasting time on evaluating time-consuming pipelines. \n",
    "<br><br>\n",
    "**random_state**: integer or None, optional (default=None)\n",
    "<br><br>\n",
    "&emsp;The seed of the pseudo random number generator used in TPOT.\n",
    "<br><br>\n",
    "&emsp;Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed. \n",
    "<br><br>\n",
    "**config_dict**: Python dictionary, string, or None, optional (default=None)\n",
    "<br><br>\n",
    "&emsp;A configuration dictionary for customizing the operators and parameters that TPOT searches in the optimization process.\n",
    "<br><br>\n",
    "&emsp;Possible inputs are:\n",
    "<br>\n",
    "&emsp;1. Python dictionary, TPOT will use your custom configuration,<br>\n",
    "&emsp;2. string 'TPOT light', TPOT will use a built-in configuration with only fast models and preprocessors, or<br>\n",
    "&emsp;3. string 'TPOT MDR', TPOT will use a built-in configuration specialized for genomic studies, or\n",
    "&emsp;4. string 'TPOT sparse': TPOT will use a configuration dictionary with a one-hot encoder and the operators normally included in TPOT that also support sparse &emsp;5. None, TPOT will use the default TPOTClassifier configuration.\n",
    "<br><br>\n",
    "&emsp;See the built-in configurations section for the list of configurations included with TPOT, and the custom configuration section for more information and examples of how to create your own TPOT configurations. \n",
    "<br><br>\n",
    "**warm_start**: boolean, optional (default=False)\n",
    "<br><br>\n",
    "&emsp;Flag indicating whether the TPOT instance will reuse the population from previous calls to fit().\n",
    "<br><br>\n",
    "&emsp;Setting warm_start=True can be useful for running TPOT for a short time on a dataset, checking the results, then resuming the TPOT run from where it left off. \n",
    "<br><br>\n",
    "**memory**: a sklearn.external.joblib.Memory object or string, optional (default=None)\n",
    "<br><br>\n",
    "&emsp;If supplied, pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. More details about memory caching in [scikit-learn documentation](http://scikit-learn.org/stable/modules/pipeline.html#caching-transformers-avoid-repeated-computation)\n",
    "<br><br>\n",
    "&emsp;Possible inputs are:\n",
    "<br><br>\n",
    "&emsp;1. String 'auto': TPOT uses memory caching with a temporary directory and cleans it up upon shutdown, or<br>\n",
    "&emsp;2. Path of a caching directory, TPOT uses memory caching with the provided directory and TPOT does NOT clean the caching directory up upon shutdown, or<br>\n",
    "&emsp;3. Memory object, TPOT uses the instance of sklearn.external.joblib.Memory for memory caching and TPOT does NOT clean the caching directory up upon shutdown, or<br>\n",
    "&emsp;None, TPOT does not use memory caching.\n",
    "<br><br>\n",
    "**periodic_checkpoint_folder**: path string, optional (default: None)\n",
    "<br>\n",
    "&emsp;If supplied, a folder in which TPOT will periodically save the best pipeline so far while optimizing.\n",
    "&emsp;Currently once per generation but not more often than once per 30 seconds.\n",
    "<br><br>\n",
    "&emsp;Useful in multiple cases:\n",
    "<br><br>\n",
    "&emsp;1. Sudden death before TPOT could save optimized pipeline<br>\n",
    "&emsp;2. Track its progress<br>\n",
    "&emsp;3. Grab pipelines while it's still optimizing<br>\n",
    "<br>\n",
    "**early_stop**: integer, optional (default: None)\n",
    "<br>\n",
    "&emsp;How many generations TPOT checks whether there is no improvement in optimization process.\n",
    "<br><br>\n",
    "&emsp;Ends the optimization process if there is no improvement in the given number of generations. \n",
    "<br><br>\n",
    "**verbosity**: integer, optional (default=0)\n",
    "<br>\n",
    "&emsp;How much information TPOT communicates while it's running.\n",
    "<br><br>\n",
    "&emsp;Possible inputs are:<br>\n",
    "<br><br>\n",
    "&emsp;0, TPOT will print nothing,<br>\n",
    "&emsp;1, TPOT will print minimal information,<br>\n",
    "&emsp;2, TPOT will print more information and provide a progress bar, or<br>\n",
    "&emsp;3, TPOT will print everything and provide a progress bar.<br>\n",
    "<br>\n",
    "**disable_update_check**: boolean, optional (default=False)\n",
    "<br>\n",
    "&emsp;Flag indicating whether the TPOT version checker should be disabled.\n",
    "<br><br>\n",
    "&emsp;The update checker will tell you when a new version of TPOT has been released. </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>Attributes:</td>\n",
    "    <td>\n",
    "**fitted_pipeline_**: scikit-learn Pipeline object\n",
    "<br>\n",
    "&emsp;The best pipeline that TPOT discovered during the pipeline optimization process, fitted on the entire training dataset. \n",
    "<br><br>\n",
    "**pareto_front_fitted_pipelines_**: Python dictionary\n",
    "<br>\n",
    "&emsp;Dictionary containing the all pipelines on the TPOT Pareto front, where the key is the string representation of the pipeline and the value is the corresponding pipeline fitted on the entire training dataset.\n",
    "<br><br>\n",
    "&emsp;The TPOT Pareto front provides a trade-off between pipeline complexity (i.e., the number of steps in the pipeline) and the predictive performance of the pipeline.\n",
    "<br><br>\n",
    "&emsp;Note: pareto_front_fitted_pipelines_ is only available when *verbosity=3*. \n",
    "<br><br>\n",
    "**evaluated_individuals_**: Python dictionary\n",
    "<br>\n",
    "&emsp;Dictionary containing all pipelines that were evaluated during the pipeline optimization process, where the key is the string representation of the pipeline and the value is a tuple containing (# of steps in pipeline, accuracy metric for the pipeline).\n",
    "<br><br>\n",
    "&emsp;This attribute is primarily for internal use, but may be useful for looking at the other pipelines that TPOT evaluated. </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "#这个fit函数初始化遗传规划算法，在平均k倍交叉验证的基础上找到得分最高的流水线，然后对所提供的样本集进行训练，TPOT实例可作为拟合模型。\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_mnist_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jupyter widget could not be displayed because the widget state could not be found. This could happen if the kernel storing the widget is no longer available, or if the widget state was not saved in the notebook. You may be able to create the widget by running the appropriate cells.\n",
    "\n",
    "Generation 1 - Current best internal CV score: 0.9725347652485199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><font color='blue'>fit</font>(features, classes[, sample_weight, groups])</td>\n",
    "        <td>Run the TPOT optimization process on the given training data.</td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td><font color='blue'>predict</font>(features)</td>\n",
    "        <td>Use the optimized pipeline to predict the classes for a feature set.</td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td><font color='blue'>predict_proba</font>(features)</td>\n",
    "        <td>Use the optimized pipeline to estimate the class probabilities for a feature set.</td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td><font color='blue'>score</font>(testing_features, testing_classes)</td>\n",
    "        <td>Returns the optimized pipeline's score on the given testing data using the user-specified scoring function.</td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td><font color='blue'>export</font>(output_file_name)</td>\n",
    "        <td>Export the optimized pipeline as Python code.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(features, classes, sample_weight=None, groups=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the TPOT optimization process on the given training data.\n",
    "\n",
    "Uses genetic programming to optimize a machine learning pipeline that maximizes the score on the provided features and target. This pipeline optimization procedure uses internal k-fold cross-validaton to avoid overfitting on the provided data. At the end of the pipeline optimization procedure, the best pipeline is then trained on the entire set of provided samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>**Parameters**:</td>\n",
    "        <td>**features**: array-like {n_samples, n_features}\n",
    "<br>\n",
    "&emsp;Feature matrix\n",
    "<br><br>\n",
    "&emsp;TPOT and all scikit-learn algorithms assume that the features will be numerical and there will be no missing values. As such, when a feature matrix is provided to TPOT, all missing values will automatically be replaced (i.e., imputed) using <font color='blue'>median value imputation</font>.\n",
    "<br><br>\n",
    "&emsp;If you wish to use a different imputation strategy than median imputation, please make sure to apply imputation to your feature set prior to passing it to TPOT. \n",
    "<br><br>\n",
    "**classes**: array-like {n_samples}\n",
    "<br><br>\n",
    "&emsp;List of class labels for prediction \n",
    "<br><br>\n",
    "**sample_weight**: array-like {n_samples}, optional\n",
    "<br>\n",
    "&emsp;Per-sample weights. Higher weights force TPOT to put more emphasis on those points. \n",
    "<br><br>\n",
    "**groups**: array-like, with shape {n_samples, }, optional\n",
    "<br>\n",
    "&emsp;Group labels for the samples used when performing cross-validation.\n",
    "<br><br>\n",
    "&emsp;This parameter should only be used in conjunction with sklearn's Group cross-validation functions, such as <font color='blue'>sklearn.model_selection.GroupKFold.</font></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>**Returns**:</td>\n",
    "        <td>**self**: object\n",
    "<br>\n",
    "&emsp;Returns a copy of the fitted TPOT object </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimized pipeline to predict the classes for a feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>**Parameters**:</td>\n",
    "        <td>**features:** array-like {n_samples, n_features}\n",
    "<br>\n",
    "&emsp;Feature matrix </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>**Returns**:</td>\n",
    "        <td>**predictions:**  array-like {n_samples}\n",
    "<br>\n",
    "&emsp;Predicted classes for the samples in the feature matrix </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimized pipeline to estimate the class probabilities for a feature set.\n",
    "\n",
    "Note: This function will only work for pipelines whose final classifier supports the *predict_proba* function. TPOT will raise an error otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>**Parameters**:</td>\n",
    "        <td>**features:** array-like {n_samples, n_features}\n",
    "<br>\n",
    "&emsp;Feature matrix </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>**Returns**:</td>\n",
    "        <td>**predictions:**  array-like {n_samples, n_features}\n",
    "<br>\n",
    "&emsp;The class probabilities of the input samples </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(testing_features, testing_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the optimized pipeline's score on the given testing data using the user-specified scoring function.\n",
    "\n",
    "The default scoring function for TPOTClassifier is 'accuracy'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>**Parameters**:</td>\n",
    "        <td>**features:** array-like {n_samples, n_features}\n",
    "<br>\n",
    "&emsp;Feature matrix of the testing set\n",
    "<br><br>\n",
    "**testing_classes:** array-like {n_samples} \n",
    "<br>\n",
    "&emsp;List of class labels for prediction in the testing set \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>**Returns**:</td>\n",
    "        <td>**accuracy_score:**float\n",
    "<br>\n",
    "&emsp;The estimated test set accuracy according to the user-specified scoring function. </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the optimized pipeline as Python code.\n",
    "\n",
    "See the [usage documentation](https://epistasislab.github.io/tpot/using/#tpot-with-code) for example usage of the export function. \n",
    "\n",
    "<table>\n",
    "\n",
    "    <tr>\n",
    "        <td>**Parameters**:</td>\n",
    "        <td>**output_file_name:** string \n",
    "<br>\n",
    "&emsp;String containing the path and file name of the desired output file \n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td>**Returns**:</td>\n",
    "        <td>Does not return anything</td>\n",
    "    </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "class tpot.TPOTRegressor(generations=100, population_size=100,\n",
    "                         offspring_size=None, mutation_rate=0.9,\n",
    "                         crossover_rate=0.1,\n",
    "                         scoring='neg_mean_squared_error', cv=5,\n",
    "                         subsample=1.0, n_jobs=1,\n",
    "                         max_time_mins=None, max_eval_time_mins=5,\n",
    "                         random_state=None, config_dict=None,\n",
    "                         warm_start=False,\n",
    "                         memory=None,\n",
    "                         periodic_checkpoint_folder=None,\n",
    "                         early_stop=None,\n",
    "                         verbosity=0,\n",
    "                         disable_update_check=False)\n",
    "\n",
    "[source](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py)\n",
    "\n",
    "Automated machine learning for supervised regression tasks.\n",
    "\n",
    "The TPOTRegressor performs an intelligent search over machine learning pipelines that can contain supervised regression models, preprocessors, feature selection techniques, and any other estimator or transformer that follows the [`scikit-learn API`](http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects). The TPOTRegressor will also search over the hyperparameters of all objects in the pipeline.\n",
    "\n",
    "By default, TPOTRegressor will search over a broad range of supervised regression models, transformers, and their hyperparameters. However, the models, transformers, and parameters that the TPOTRegressor searches over can be fully customized using the `config_dict` parameter.\n",
    "\n",
    "Read more in the [User Guide](https://epistasislab.github.io/tpot/using/#tpot-with-code).\n",
    "\n",
    "[Parameters](https://epistasislab.github.io/tpot/api/)\n",
    "[Attributes](https://epistasislab.github.io/tpot/api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions\n",
    "<br>\n",
    "[Functions](https://epistasislab.github.io/tpot/api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(features, target, sample_weight=None, groups=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the TPOT optimization process on the given training data.\n",
    "\n",
    "Uses genetic programming to optimize a machine learning pipeline that maximizes the score on the provided features and target. This pipeline optimization procedure uses internal k-fold cross-validaton to avoid overfitting on the provided data. At the end of the pipeline optimization procedure, the best pipeline is then trained on the entire set of provided samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parameters](https://epistasislab.github.io/tpot/api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimized pipeline to predict the target values for a feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(testing_features, testing_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the optimized pipeline's score on the given testing data using the user-specified scoring function.\n",
    "\n",
    "The default scoring function for TPOTClassifier is 'mean_squared_error'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the optimized pipeline as Python code.\n",
    "\n",
    "See the [`usage documentation`](https://epistasislab.github.io/tpot/using/#tpot-with-code) for example usage of the export function. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
