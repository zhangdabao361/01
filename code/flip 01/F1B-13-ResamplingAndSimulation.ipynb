{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Data Science \n",
    "**(Module 03: Pattern Classification)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Session N - Resampling And Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling and Monte Carlo Simulations\n",
    "====\n",
    "\n",
    "Broadly, any simulation that relies on random sampling to obtain results falls into the category of Monte Carlo methods. Another common type of statistical experiment is the use of repeated sampling from a data set, including the bootstrap, jackknife and permutation resampling. Often, they are combined, as when we use a random set of permutations rather than the full set of permutations, which grows as $O(n!))$ and is typically infeasible. What Monte Carlo simulations have in common is that they are typically more flexible but also more computationally demanding than methods based on asymptotic results. Because of their flexibility and the inexorable growth of computing power, I expect these computational simulation methods to only become more popular over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the random seed\n",
    "\n",
    "In any probabilistic simulation, it is prudent to set the random number seed so that results can be replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with and without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sampling is done with replacement by default\n",
    "np.random.choice(4, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probability weights can be given\n",
    "np.random.choice(4, 12, p=[.4, .1, .1, .4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10, (8, 12))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampling individual elements\n",
    "np.random.choice(x.ravel(), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampling rows\n",
    "idx = np.random.choice(x.shape[0], 4)\n",
    "x[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampling columns\n",
    "idx = np.random.choice(x.shape[1], 4)\n",
    "x[:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Give the argument replace=False\n",
    "try:\n",
    "    np.random.choice(4, 12, replace=False)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffling occurs \"in place\" for efficiency\n",
    "np.random.shuffle(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To shuffle columns instead, transpose before shuffling\n",
    "np.random.shuffle(x.T)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# numpy.random.permutation does the same thing but returns a copy\n",
    "np.random.permutation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# When given an integre n, permutation treats is as the array arange(n)\n",
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use indices if you needed to shuffle collections of arrays in synchrony\n",
    "x = np.arange(12).reshape(4,3)\n",
    "y = x + 10\n",
    "idx = np.random.permutation(x.shape[0])\n",
    "list(zip(x[idx, :], y[idx, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "The bootstrap is commonly used to estimate statistics when a closed form solution may not exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For example, what is the 95% confidence interval for \n",
    "# the 10th percentile of this data set if you didn't know how it was generated?\n",
    "\n",
    "x = np.concatenate([np.random.exponential(size=200), np.random.normal(size=100)])\n",
    "plt.hist(x, 25, histtype='step', linewidth=1)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "reps = 10000\n",
    "xb = np.random.choice(x, (n, reps))\n",
    "mb = np.percentile(xb, 10, axis=0)\n",
    "mb.sort()\n",
    "\n",
    "lower, upper = np.percentile(mb, [2.5, 97.5])\n",
    "sns.kdeplot(mb)\n",
    "for v in (lower, upper):\n",
    "    plt.axvline(v, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For flexible hypothesis testing\n",
    "\n",
    "Suppose you have two data sets from unknown distributions and you want to test if some arbitrary statistic (e.g the 7th percentile) is the same in the two data sets - what can you do?\n",
    "\n",
    "An appropriate test statistic is the difference between the 7th percentile, and if we knew the null distribution of this statistic, we could test for the null hypothesis that the statistic = 0. Permuting the labels of the 2 data sets allows us to create the empirical null distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create two data sets for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.r_[np.random.exponential(size=200), \n",
    "          np.random.normal(0, 1, size=100)]\n",
    "y = np.r_[np.random.exponential(size=250), \n",
    "          np.random.normal(0, 1, size=50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate permutations of labels for 10,000 comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n1, n2 = map(len, (x, y))\n",
    "reps = 10000\n",
    "\n",
    "data = np.r_[x, y]\n",
    "ps = np.array([np.random.permutation(n1+n2) for i in range(reps)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate empirical null distribution for differences between samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = data[ps[:, :n1]]\n",
    "yp = data[ps[:, n1:]]\n",
    "samples = np.percentile(xp, 7, axis=1) - np.percentile(yp, 7, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(samples, 25, histtype='step', color='red')\n",
    "test_stat = np.percentile(x, 7) - np.percentile(y, 7)\n",
    "plt.axvline(test_stat)\n",
    "plt.axvline(np.percentile(samples, 2.5), linestyle='--')\n",
    "plt.axvline(np.percentile(samples, 97.5), linestyle='--')\n",
    "print(\"p-value =\", 2*np.sum(samples >= np.abs(test_stat))/reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting p-values for multiple testing\n",
    "\n",
    "We will make up some data - a typical example is trying to identify genes that are differentially expressed in two groups of people, perhaps those who are healthy and those who are sick. For each gene, we can perform a t-test to see if the gene is differentially expressed across the two groups at some nominal significance level, typically 0.05. When we have many genes, this is unsatisfactory since 5% of the genes will be found to be differentially expressed just by chance.\n",
    "\n",
    "One possible solution is to use the family-wise error rate (FWER) instead - most simply using the Bonferroni adjusted p-value. An alternative is to use the non-parametric method originally proposed by Young and Westfall that uses permutation resampling to estimate the adjusted p-value without the assumptions of independence that the Bonferroni method makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Making up data for 100 genes across 1000 subjects and  \"spike\" with 5 genes that are differentially expressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3]).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('this is x', x)\n",
    "print('This is x1',  x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x @ x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngenes = 100\n",
    "ncases = 500\n",
    "nctrls = 500\n",
    "nsamples = ncases + nctrls\n",
    "x = np.random.normal(0, 1, (ngenes, nsamples))\n",
    "\n",
    "target_genes = [5,15,25,35,45]\n",
    "x[target_genes, ncases:] += np.random.normal(1, 1, (len(target_genes), ncases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unadjusted p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t, p0 = stats.ttest_ind(x[:, :ncases], x[:, ncases:], axis=1)\n",
    "idx = p0 < 0.05\n",
    "list(zip(np.nonzero(idx)[0], p0[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vmin = x.min()\n",
    "vmax = x.max()\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[:, :ncases], extent=[0, 1, 0, 2], interpolation='nearest', \n",
    "           vmin=vmin, vmax=vmax, cmap='jet')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Controls')\n",
    "plt.subplot(122)\n",
    "plt.imshow(x[:, ncases:], extent=[0, 1, 0, 2], interpolation='nearest', \n",
    "           vmin=vmin, vmax=vmax, cmap='jet')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Cases')\n",
    "plt.colorbar()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonferroni correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p1 = np.clip(len(p0) * p0, 0, 1)\n",
    "idx = p1 < 0.05\n",
    "list(zip(np.nonzero(idx)[0], p1[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Westfall and Young (Permutation-resampling based family-wise error rate)\n",
    "\n",
    "Is similar to Bonferroni when features are uncorrelated, but is more powerful when features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nperms = 10000\n",
    "k = ngenes\n",
    "\n",
    "t, p0 = stats.ttest_ind(x[:, :ncases], x[:, ncases:], axis=1)\n",
    "ranks = np.argsort(np.abs(t))[::-1]\n",
    "counts = np.zeros((nperms, k))\n",
    "for i in range(nperms):\n",
    "    u = np.zeros(k)\n",
    "    sidx = np.random.permutation(nsamples)\n",
    "    y = x[:, sidx]\n",
    "    tb, pb = stats.ttest_ind(y[:, :ncases], y[:, ncases:], axis=1)\n",
    "    u[k-1] = np.abs(tb[ranks[k-1]])\n",
    "    for j in range(k-2, -1, -1):\n",
    "        u[j] = max(u[j+1], np.abs(tb[ranks[j]]))\n",
    "    counts[i] = (u >= np.abs(t[ranks]))\n",
    "\n",
    "p2 = np.sum(counts, axis=0)/nperms\n",
    "for i in range(1, k):\n",
    "    p2[i] = max(p2[i],p2[i-1])\n",
    "idx = p2 < 0.05\n",
    "list(zip(ranks, p2[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sorted(p0), label='No correction')\n",
    "plt.plot(sorted(p1), label='Bonferroni')\n",
    "plt.plot(sorted(p2), label='Westfall-Young')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(loc='best')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if genes are correlated?\n",
    "\n",
    "The Bonferrroni assumes that tests are independent. However, often test results are strongly correlated (e.g. genes in the same pathway behave similarly) and the Bonferroni will be too conservative. However the permutation-resampling method still works in the presence of correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngenes = 100\n",
    "ncases = 500\n",
    "nctrls = 500\n",
    "nsamples = ncases + nctrls\n",
    "\n",
    "# use random number seed knwon to give a differnece\n",
    "np.random.seed(52)\n",
    "x = np.repeat(np.random.normal(0, 1, (1, nsamples)), ngenes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In this extreme case, we measure the same gene 100 times\n",
    "x[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unadjusted p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t, p0 = stats.ttest_ind(x[:, :ncases], x[:, ncases:], axis=1)\n",
    "idx = p0 < 0.05\n",
    "print('Minimum p-value', p0.min(), '# significant', idx.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonferroni\n",
    "\n",
    "Bonferroni tells us none of the adjusted p-values are significant, which we know is the wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p1 = np.clip(len(p0) * p0, 0, 1)\n",
    "idx = p1 < 0.05\n",
    "print('Minimum p-value', p1.min(), '# significant', idx.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Westfall and Young (Permutation-resampling based family-wise error rate)\n",
    "\n",
    "This tells us that every gene is significant, which is the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nperms = 10000\n",
    "\n",
    "counts = np.zeros((nperms, k))\n",
    "t, p0 = stats.ttest_ind(x[:, :ncases], x[:, ncases:], axis=1)\n",
    "ranks = np.argsort(np.abs(t))[::-1]\n",
    "for i in range(nperms):\n",
    "    u = np.zeros(k)\n",
    "    sidx = np.random.permutation(nsamples)\n",
    "    y = x[:, sidx]\n",
    "    tb, pb = stats.ttest_ind(y[:, :ncases], y[:, ncases:], axis=1)\n",
    "    u[k-1] = np.abs(tb[ranks[k-1]])\n",
    "    for j in range(k-2, -1, -1):\n",
    "        u[j] = max(u[j+1], np.abs(tb[ranks[j]]))\n",
    "    counts[i] = (u >= np.abs(t[ranks]))\n",
    "\n",
    "p2 = np.sum(counts, axis=0)/nperms\n",
    "for i in range(1, k):\n",
    "    p2[i] = max(p2[i],p2[i-1])\n",
    "idx = p2 < 0.05\n",
    "    \n",
    "print ('Minimum p-value', p2.min(), '# significant', idx.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sorted(p1), label='Bonferroni')\n",
    "plt.plot(sorted(p2), label='Westfall-Young')\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.legend(loc='best')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Leave one out\" resampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jackknife estimate of parameters\n",
    "\n",
    "This shows the leave-one-out calculation idiom for Python. Unlike R, a -k index to an array does not delete the kth entry, but returns the kth entry from the end, so we need another way to efficiently drop one scalar or vector. This can be done using Boolean indexing as shown in the examples below, and is efficient since the operations are on *views* of the original array rather than *copies*. Note also that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jackknife(x, func):\n",
    "    \"\"\"Jackknife estimate of the estimator func\"\"\"\n",
    "    n = len(x)\n",
    "    idx = np.arange(n)\n",
    "    return np.sum(func(x[idx!=i]) for i in range(n))/float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Jackknife estimate of standard deviation\n",
    "x = np.random.normal(0, 2, 100)\n",
    "jackknife(x, np.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jackknife_var(x, func):\n",
    "    \"\"\"Jackknife estiamte of the variance of the estimator func.\"\"\"\n",
    "    n = len(x)\n",
    "    idx = np.arange(n)\n",
    "    j_est = jackknife(x, func)\n",
    "    return (n-1)/(n + 0.0) * np.sum((func(x[idx!=i]) - j_est)**2.0 for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# estimate of the variance of an estimator \n",
    "jackknife_var(x, np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave one out cross validation (LOOCV)\n",
    "\n",
    "LOOCV also uses the same idiom, and a simple example of LOOCV for model selection is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b, c = 2, 3, 4\n",
    "x = np.linspace(0, 5, 6)\n",
    "y = a*x**2 + b*x + c + np.random.normal(0, 3, len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "for deg in range(1, 6):\n",
    "    plt.subplot(1, 6, deg)\n",
    "    beta = np.polyfit(x, y, deg)\n",
    "    plt.plot(x, y, 'r:o')\n",
    "    plt.plot(x, np.polyval(beta, x), 'b-')\n",
    "    plt.title('Degree = %d' % deg)\n",
    "    plt.margins(0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loocv(x, y, fit, pred, deg):\n",
    "    \"\"\"LOOCV RSS for fitting a polynomial model.\"\"\"\n",
    "    n = len(x)\n",
    "    idx = np.arange(n)\n",
    "    rss = np.sum([(y - pred(fit(x[idx!=i], y[idx!=i], deg), x))**2.0 for i in range(n)])\n",
    "    return rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RSS does not detect overfitting and selects the most complex model\n",
    "for deg in range(1, 6):\n",
    "    print('Degree = %d, RSS=%.2f' % (deg, np.sum((y - np.polyval(np.polyfit(x, y, deg), x))**2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOOCV selects a more conservative model\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    for deg in range(1, 6):\n",
    "        print('Degree = %d, RSS=%.2f' % (deg, loocv(x, y, np.polyfit, np.polyval, deg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulations to estimate power\n",
    "----\n",
    "\n",
    "What sample size is needed for the t-test to have a power of 0.8 with an effect size of 0.5?\n",
    "\n",
    "This is a toy example, since you can calculate it exactly,  but the simulation approach works for *everything*, including arbitrarily complex experimental designs, correcting for multiple comparisons and so on (assuming infinite computational resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run nresps simulations\n",
    "# The power is simply the fraction of reps where \n",
    "# the p-value is less than 0.05\n",
    "\n",
    "nreps = 10000\n",
    "d = 0.5\n",
    "\n",
    "n = 50\n",
    "power = 0\n",
    "while power < 0.8:\n",
    "    n1 = n2 = n\n",
    "    x = np.random.normal(0, 1, (n1, nreps))\n",
    "    y = np.random.normal(d, 1, (n2, nreps))\n",
    "    t, p = stats.ttest_ind(x, y)\n",
    "    power = (p < 0.05).sum()/nreps\n",
    "    print(n, power)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check with R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(pwr)\n",
    "\n",
    "power.t.test(sig.level=0.05, power=0.8, delta = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating CDF and PDF from Monte Carlo samples\n",
    "\n",
    "Given a bunch of random numbers from a simulation experiment, one of the first steps is to visualize the CDF and PDF. The ECDF is quite useful for, say, visualizing how similar or different two sets of data are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make up some random data\n",
    "x = np.r_[np.random.normal(0, 1, 10000), \n",
    "          np.random.normal(4, 1, 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Roll our own ECDF function\n",
    "\n",
    "def ecdf(x):\n",
    "    \"\"\"Return empirical CDF of x.\"\"\"\n",
    "    \n",
    "    sx = np.sort(x)\n",
    "    cdf = (1.0 + np.arange(len(sx)))/len(sx)\n",
    "    return sx, cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sx, y = ecdf(x)\n",
    "plt.plot(sx, y)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using library routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "ecdf = ECDF(x)\n",
    "plt.plot(ecdf.x, ecdf.y)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the PDF\n",
    "\n",
    "The simplest is to plot a normalized histogram as shown above, but we will also look at how to estimate density functions using kernel density estimation (KDE). KDE works by placing a kernel unit on each data point, and summing the kernels to present a smoother estimate than you would get with a (n-d) histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epanechnikov(u):\n",
    "    \"\"\"Epanechnikov kernel.\"\"\"\n",
    "    return np.where(np.abs(u) <= np.sqrt(5), 3/(4*np.sqrt(5)) * (1 - u*u/5.0), 0)\n",
    "\n",
    "def silverman(y):\n",
    "    \"\"\"Find bandwidth using heuristic suggested by Silverman\n",
    "    .9 min(standard deviation, interquartile range/1.34)n−1/5\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    iqr = np.subtract(*np.percentile(y, [75, 25]))\n",
    "    h = 0.9*np.min([y.std(ddof=1), iqr/1.34])*n**-0.2\n",
    "    return h\n",
    "\n",
    "def kde(x, y, bandwidth=silverman, kernel=epanechnikov):\n",
    "    \"\"\"Returns kernel density estimate.\n",
    "    x are the points for evaluation\n",
    "    y is the data to be fitted\n",
    "    bandwidth is a function that returens the smoothing parameter h\n",
    "    kernel is a function that gives weights to neighboring data\n",
    "    \"\"\"\n",
    "    h = bandwidth(y)\n",
    "    return np.sum(kernel((x-y[:, None])/h)/h, axis=0)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(-5,8,100)\n",
    "density = kde(xs, x)\n",
    "plt.plot(xs, density)\n",
    "xlim = plt.xlim()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or just use `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(x, kernel='epa', bw='silverman', shade=True)\n",
    "plt.xlim(xlim)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other library routines\n",
    "\n",
    "There are several kernel density estimation routines available in `scipy`, `statsmodels` and `scikit-leran`. Here we will use the `scikits-learn` and `statsmodels` routine as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "dens = sm.nonparametric.KDEUnivariate(x)\n",
    "dens.fit(kernel='gau')\n",
    "plt.plot(xs, dens.evaluate(xs))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity \n",
    "\n",
    "# expects n x p matrix with p features\n",
    "x.shape = (len(x), 1)\n",
    "xs.shape = (len(xs), 1)\n",
    "\n",
    "kde = KernelDensity(kernel='epanechnikov').fit(x)\n",
    "dens = np.exp(kde.score_samples(xs))\n",
    "plt.plot(xs, dens)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
