{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Data Science \n",
    "**(Module 03: Pattern Classification)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Session A - Bayesian \n",
    "\n",
    "In this session, it contains four parts contents: the first part is estimating model parameters, the second part is model checking, the third part is hierarchal modeling, the fourth part is bayesian regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Estimating model parameters\n",
    "In this part we will discuss how Bayesians think about data, and how we can estimate model parameters using a technique called MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as opt\n",
    "from statsmodels import api as sm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "colors = ['#348ABD', '#A60628', '#7A68A6', '#467821', '#D55E00', \n",
    "          '#CC79A7', '#56B4E9', '#009E73', '#F0E442', '#0072B2']\n",
    "\n",
    "messages = pd.read_csv('data/hangout_chat_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do Bayesians think about data?\n",
    "When I started to learn how to apply Bayesian methods, I found it very useful to understand how Bayesians think about data. Imagine the following scenario:\n",
    "> A curious boy watches the number of cars that pass by his house every day. He diligently notes down the total count of cars that pass per day. Over the past week, his notebook contains the following counts: 12, 33, 20, 29, 20, 30, 18\n",
    "\n",
    "From a Bayesian's perspective, this data is generated by a random process. However, now that the data is observed, it is fixed and does not change. This random process has some model parameters that are fixed. However, the Bayesian uses probability distributions to represent his/her uncertainty in these parameters.\n",
    "\n",
    "Because the boy is measuring counts (non-negative integers), it is common practice to use a Poisson distribution to model the data (eg. the random process). A Poisson distribution takes a single parameter $\\mu$ which describes both the mean and variance of the data. You can see 3 Poisson distributions below with different values of $\\mu$.\n",
    "\n",
    "$$p(x \\ | \\ \\mu) = \\frac{e^{-\\mu}\\mu^{x}} {x!} \\mbox{    for     } \n",
    "x = 0, 1, 2, \\cdots$$\n",
    "\n",
    "$$\\lambda = E(x) = Var(\\mu)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,3))\n",
    "ax = fig.add_subplot(111)\n",
    "x_lim = 60\n",
    "mu = [5, 20, 40]\n",
    "for i in np.arange(x_lim):\n",
    "    plt.bar(i, stats.poisson.pmf(mu[0], i), color=colors[3])\n",
    "    plt.bar(i, stats.poisson.pmf(mu[1], i), color=colors[4])\n",
    "    plt.bar(i, stats.poisson.pmf(mu[2], i), color=colors[5])\n",
    "    \n",
    "_ = ax.set_xlim(0, x_lim)\n",
    "_ = ax.set_ylim(0, 0.2)\n",
    "_ = ax.set_ylabel('Probability mass')\n",
    "_ = ax.set_title('Poisson distribution')\n",
    "_ = plt.legend(['$\\mu$ = %s' % mu[0], '$\\mu$ = %s' % mu[1], '$\\mu$ = %s' % mu[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we imported my hangout chat dataset. I'm particularly interested in the time it takes me to respond to messages (`response_time`). Given that `response_time` is count data, we can model it as a Poisson distribution and estimate its parameter $\\mu$. We will explore both a frequentist and Bayesian method of estimating this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,3))\n",
    "_ = plt.title('Frequency of messages by response time')\n",
    "_ = plt.xlabel('Response time (seconds)')\n",
    "_ = plt.ylabel('Number of messages')\n",
    "_ = plt.hist(messages['time_delay_seconds'].values, \n",
    "             range=[0, 60], bins=60, histtype='stepfilled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentists method of estimating $\\mu$\n",
    "Before we jump into Bayesian techniques, lets first look at a frequentist method of estimating the parameters of a Poisson distribution. We will use an optimization technique that aims to maximize the likelihood of a function.\n",
    "\n",
    "The below function `poisson_logprob()` returns the overall likelihood of the observed data given a Poisson model and parameter value. We use the method `opt.minimize_scalar` to find the value of $\\mu$ that is most credible (maximizes the log likelihood) given the data observed. Under the hood, this optimization technique is intelligently iterating through possible values of `mu` until it finds a value with the highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_obs = messages['time_delay_seconds'].values\n",
    "\n",
    "def poisson_logprob(mu, sign=-1):\n",
    "    return np.sum(sign*stats.poisson.logpmf(y_obs, mu=mu))\n",
    "\n",
    "freq_results = opt.minimize_scalar(poisson_logprob)\n",
    "%time print(\"The estimated value of mu is: %s\" % freq_results['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the estimate of the value of $\\mu$ is 18.0413533867. The optimization technique doesn't provide any measure of uncertainty - it just returns a point value. And it does so very efficiently...\n",
    "\n",
    "The below plot illustrates the function that we are optimizing. At each value of $\\mu$, the plot shows the log probability at $\\mu$ given the data and the model. The optimizer works in a hill climbing fashion - starting at a random point on the curve and incrementally climbing until it cannot get to a higher point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1, 60)\n",
    "y_min = np.min([poisson_logprob(i, sign=1) for i in x])\n",
    "y_max = np.max([poisson_logprob(i, sign=1) for i in x])\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "_ = plt.plot(x, [poisson_logprob(i, sign=1) for i in x])\n",
    "_ = plt.fill_between(x, [poisson_logprob(i, sign=1) for i in x], \n",
    "                     y_min, color=colors[0], alpha=0.3)\n",
    "_ = plt.title('Optimization of $\\mu$')\n",
    "_ = plt.xlabel('$\\mu$')\n",
    "_ = plt.ylabel('Log probability of $\\mu$ given data')\n",
    "_ = plt.vlines(freq_results['x'], y_max, y_min, colors='red', linestyles='dashed')\n",
    "_ = plt.scatter(freq_results['x'], y_max, s=110, c='red', zorder=3)\n",
    "_ = plt.ylim(ymin=y_min, ymax=0)\n",
    "_ = plt.xlim(xmin=1, xmax=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above optimization has estimated the parameter ($\\mu$) of a Poisson model to be 18. We know for any Poisson distribution, the parameter $\\mu$ represents both its mean and variance. The below plot illustrates this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,3))\n",
    "ax = fig.add_subplot(111)\n",
    "x_lim = 60\n",
    "mu = np.int(freq_results['x'])\n",
    "for i in np.arange(x_lim):\n",
    "    plt.bar(i, stats.poisson.pmf(mu, i), color=colors[3])\n",
    "    \n",
    "_ = ax.set_xlim(0, x_lim)\n",
    "_ = ax.set_ylim(0, 0.1)\n",
    "_ = ax.set_xlabel('Response time in seconds')\n",
    "_ = ax.set_ylabel('Probability mass')\n",
    "_ = ax.set_title('Estimated Poisson distribution for Hangout chat response time')\n",
    "_ = plt.legend(['$\\lambda$ = %s' % mu])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above Poisson model and estimated value of $\\mu$ suggest that there is minimal chance of an observation less than 10 or greater than 30. The vast majority of the probability mass is between 10 and 30. However, we know this is not reflected in the data that we observed - which has observed values between 0 and 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian method of estimating $\\mu$\n",
    "\n",
    "If you've encountered Bayes' theorem before, the below formula will look familiar. This framework never resonated with me until I read John K. Kruschke's book \"Doing Bayesian Data Analysis\" and saw the below formula through the lens of his beautifully simple Bayesian graphical models.\n",
    " \n",
    "$$\\overbrace{p(\\mu \\ |\\ Data)}^{\\text{posterior}} = \\dfrac{\\overbrace{p(Data \\ | \\ \\mu)}^{\\text{likelihood}} \\cdot \\overbrace{p(\\mu)}^{\\text{prior}}}{\\underbrace{p(Data)}_{\\text{marginal likelihood}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image('image/Poisson-dag.png', width=320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above schema can be interpreted as follows (from the bottom up):\n",
    "- We observe counts of data (y) for each conversation i (Observed Data)\n",
    "- This data was generated by a random process which we believe can be represented as a Poisson distribution (Likelihood)\n",
    "- This Poisson distribution has a single parameter $\\mu$ which we know is between 0 and 60 (Prior)\n",
    "  - We will model $\\mu$ as a uniform distribution because we do not have an opinion as to where within this range to expect it\n",
    "\n",
    "### The magical mechanics of MCMC\n",
    "The process of Markov Chain Monte Carlo (MCMC) is nicely illustrated in the below animation. The MCMC sampler draws parameter values from the prior distribution and computes the likelihood that the observed data came from a distribution with these parameter values. \n",
    "\n",
    "$$\\overbrace{p(\\mu \\ |\\ Data)}^{posterior} \\varpropto \\overbrace{p(Data \\ | \\ \\mu)}^{likelihood} \\cdot \\overbrace{p(\\mu)}^{prior}$$\n",
    "\n",
    "This calculation acts as a guiding light for the MCMC sampler. As it draws values from the paramater priors, it computes the likelihood of these paramters given the data - and will try to guide the sampler towards areas of higher probability.\n",
    "\n",
    "In a conceptually similar manner to the frequentist optimization technique discussed above, the MCMC sampler wanders towards areas of highest likelihood. However, the Bayesian method is not concerned with findings the absolute maximum values - but rather to traverse and collect samples around the area of highest probability. All of the samples collected are considered to be a credible parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url='image/mcmc-animate.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Uniform('mu', lower=0, upper=60)\n",
    "    likelihood = pm.Poisson('likelihood', mu=mu, observed=messages['time_delay_seconds'].values)\n",
    "    \n",
    "    start = pm.find_MAP()\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(200000, step, start=start, progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code has just gathered 200,000 credible samples of $\\mu$ by traversing over the areas of high likelihood of the posterior distribution of $\\mu$. The below plot (left) shows the distribution of values collected for $\\mu$. The mean of this distribution is almost identical to the frequentist estimate (red line). However, we also get a measure of uncertainty and can see that there are credible values of $\\mu$ between 17 and 19. This measure of uncertainty is incredibly valuable as we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = pm.traceplot(trace, varnames=['mu'], lines={'mu': freq_results['x']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discarding early samples (burnin)\n",
    "You may have wondered what the purpose of `pm.find_MAP()` is in the above MCMC code. MAP stands for maximum a posteriori estimation. It helps the MCMC sampler find a good place from which to start sampling. Ideally this will start the model off in an area of high likelihood - but sometimes that doesn't happen. As a result, the samples collected early in the trace (burnin samples) are often discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,3))\n",
    "plt.subplot(121)\n",
    "_ = plt.title('Burnin trace')\n",
    "_ = plt.ylim(ymin=16.5, ymax=19.5)\n",
    "_ = plt.plot(trace.get_values('mu')[:1000])\n",
    "fig = plt.subplot(122)\n",
    "_ = plt.title('Full trace')\n",
    "_ = plt.ylim(ymin=16.5, ymax=19.5)\n",
    "_ = plt.plot(trace.get_values('mu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model convergence\n",
    "#### Trace\n",
    "Just because the above model estimated a value for $\\mu$, doesn't mean the model estimated a good value given the data. There are some recommended checks that you can make. Firstly, look at the trace output. You should see the trace jumping around and generally looking like a hairy caterpillar. If you see the trace snake up and down or appear to be stuck in any one location - it is a sign that you have convergence issues and the estimations from the MCMC sampler cannot be trusted.\n",
    "\n",
    "#### Autocorrelation plot\n",
    "The second test you can perform is the autocorrelation test (see below plot). It is a measure of correlation between successive samples in the MCMC sampling chain. When samples have low correlation with each other, they are adding more \"information\" to the estimate of your parameter value than samples that are highly correlated.\n",
    "\n",
    "Visually, you are looking for an autocorrelation plot that tapers off to zero relatively quickly and then oscilates above and below zero correlation. If your autocorrelation plot does not taper off - it is generally a sign of poor mixing and you should revisit your model selection (eg. likelihood) and sampling methods (eg. Metropolis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = pm.autocorrplot(trace[:2000], varnames=['mu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Model checking\n",
    "In this part, we will look at two techniques that aim to answer:\n",
    "1. Are the model and parameters estimated a good fit for the underlying data?\n",
    "2. Given two separate models, which is a better fit for the underlying data?\n",
    "\n",
    "----\n",
    "\n",
    "### Model Check I: Posterior predictive check\n",
    "One method of checking model fit is called the posterior predictive check. I find this to be a very intuitive technique. You'll recall in the previous section we estimated the parameter $\\mu$ of a Poisson distribution by collecting 200,000 samples from the posterior distribution of $\\mu$. Each of these samples was considered to be a credible parameter value.\n",
    "\n",
    "The posterior predictive check requires one to generate new data from the predicted model. What does that mean? Well, we have estimated 200,000 credible values of $\\mu$ for the Poisson distribution. That means we can construct 200,000 Poisson distributions with these values and then randomly sample from these distributions. This is formally represented as:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Conceptually, if the model is a good fit for the underlying data - then the generated data should resemble the original observed data. PyMC provides a convenient way to sample from the fitted model. You may have noticed a new line in the above model specification: \n",
    "\n",
    "`y_pred = pm.Poisson('y_pred', mu=mu)`\n",
    "\n",
    "This is almost identical to `y_est` except we do not specify the observed data. PyMC considers this to be a stochastic node (as opposed to an observed node) and as the MCMC sampler runs - it also samples data from `y_est`.\n",
    "\n",
    "We then plot `y_pred` below and compare it to the observed data `y_est`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import theano.tensor as tt\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "colors = ['#348ABD', '#A60628', '#7A68A6', '#467821', '#D55E00', \n",
    "          '#CC79A7', '#56B4E9', '#009E73', '#F0E442', '#0072B2']\n",
    "\n",
    "messages = pd.read_csv('data/hangout_chat_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Uniform('mu', lower=0, upper=100)\n",
    "    y_est = pm.Poisson('y_est', mu=mu, observed=messages['time_delay_seconds'].values)\n",
    "\n",
    "    y_pred = pm.Poisson('y_pred', mu=mu)\n",
    "    \n",
    "    start = pm.find_MAP()\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(200000, step, start=start, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_lim = 60\n",
    "burnin = 50000\n",
    "\n",
    "y_pred = trace[burnin:].get_values('y_pred')\n",
    "mu_mean = trace[burnin:].get_values('mu').mean()\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "fig.add_subplot(211)\n",
    "\n",
    "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype='stepfilled', color=colors[1])   \n",
    "_ = plt.xlim(1, x_lim)\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Posterior predictive distribution')\n",
    "\n",
    "fig.add_subplot(212)\n",
    "\n",
    "_ = plt.hist(messages['time_delay_seconds'].values, range=[0, x_lim], bins=x_lim, histtype='stepfilled')\n",
    "_ = plt.xlabel('Response time in seconds')\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Distribution of observed data')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right distribution\n",
    "I'm not particularly happy with the above plot. Ideally, I'd like the posterior predictive distribution to somewhat resemble the distribution of the observed data. Intuitively, if we have correctly estimated the parameters of the model, then we should be able to sample similar data from that model. Clearly this is not the case.\n",
    "\n",
    "Perhaps the Poisson distribution is not suitable for this data. One alternative option we have is the Negative Binomial distribution. This has very similar characteristics to the Poisson distribution except that it has two parameters ($\\mu$ and $\\alpha$) which enables it to vary its variance independently of its mean. Recall that the Poisson distribution has one parameter ($\\mu$) that represents both its mean and its variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.add_subplot(211)\n",
    "x_lim = 70\n",
    "mu = [15, 40]\n",
    "for i in np.arange(x_lim):\n",
    "    plt.bar(i, stats.poisson.pmf(mu[0], i), color=colors[3])\n",
    "    plt.bar(i, stats.poisson.pmf(mu[1], i), color=colors[4])\n",
    "    \n",
    "_ = plt.xlim(1, x_lim)\n",
    "_ = plt.xlabel('Response time in seconds')\n",
    "_ = plt.ylabel('Probability mass')\n",
    "_ = plt.title('Poisson distribution')\n",
    "_ = plt.legend(['$\\lambda$ = %s' % mu[0],\n",
    "                '$\\lambda$ = %s' % mu[1]])\n",
    "\n",
    "# Scipy takes parameters n & p, not mu & alpha\n",
    "def get_n(mu, alpha):\n",
    "    return 1. / alpha * mu\n",
    "\n",
    "def get_p(mu, alpha):\n",
    "    return get_n(mu, alpha) / (get_n(mu, alpha) + mu)\n",
    "\n",
    "fig.add_subplot(212)\n",
    "\n",
    "a = [2, 4]\n",
    "\n",
    "for i in np.arange(x_lim):\n",
    "    plt.bar(i, stats.nbinom.pmf(i, n=get_n(mu[0], a[0]), p=get_p(mu[0], a[0])), color=colors[3])\n",
    "    plt.bar(i, stats.nbinom.pmf(i, n=get_n(mu[1], a[1]), p=get_p(mu[1], a[1])), color=colors[4])\n",
    "\n",
    "_ = plt.xlabel('Response time in seconds')\n",
    "_ = plt.ylabel('Probability mass')\n",
    "_ = plt.title('Negative Binomial distribution')\n",
    "_ = plt.legend(['$\\\\mu = %s, \\/ \\\\beta = %s$' % (mu[0], a[0]),\n",
    "                '$\\\\mu = %s, \\/ \\\\beta = %s$' % (mu[1], a[1])])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go ahead and estimate the parameters for a Negative Binomial distribution given the same dataset used before. Again, we will use a Uniform distribution to estimate both $\\mu$ and $\\alpha$. The model can be represented as:\n",
    "\n",
    "$$y_{j} \\sim NegBinomial(\\mu, \\alpha)$$\n",
    "$$\\alpha = Exponential(0.2)$$\n",
    "$$\\mu = Uniform(0,100)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image('image/Neg Binomial Dag.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    alpha = pm.Exponential('alpha', lam=.2)\n",
    "    mu = pm.Uniform('mu', lower=0, upper=100)\n",
    "    \n",
    "    y_pred = pm.NegativeBinomial('y_pred', mu=mu, alpha=alpha)\n",
    "    y_est = pm.NegativeBinomial('y_est', mu=mu, alpha=alpha, observed=messages['time_delay_seconds'].values)\n",
    "    \n",
    "    start = pm.find_MAP()\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(200000, step, start=start, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = pm.traceplot(trace[burnin:], varnames=['alpha', 'mu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the above model has greater uncertainty around the estimation of the mean response time $(\\mu)$ for chat messages:\n",
    "- Poisson: 10 to 30\n",
    "- Negative Binomial: 16 to 21\n",
    "\n",
    "Additionally, the Negative Binonomial model has an $\\alpha$ parameter of 1.2 to 2.2 which further increases the variance in the estimated parameter $\\mu$. Let's have a look at the posterior preditive distribution and see if it more closely resembles the distribution from the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_lim = 60\n",
    "y_pred = trace[burnin:].get_values('y_pred')\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "fig.add_subplot(211)\n",
    "\n",
    "fig.add_subplot(211)\n",
    "\n",
    "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype='stepfilled', color=colors[1])   \n",
    "_ = plt.xlim(1, x_lim)\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Posterior predictive distribution')\n",
    "\n",
    "fig.add_subplot(212)\n",
    "\n",
    "_ = plt.hist(messages['time_delay_seconds'].values, range=[0, x_lim], bins=x_lim, histtype='stepfilled')\n",
    "_ = plt.xlabel('Response time in seconds')\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Distribution of observed data')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, these two distributions are looking more similar to one another. As per the posterior predictive check, this would suggest that the Negative binomial model is a more appropriate fit for the underlying data. \n",
    "\n",
    "If you find yourself doubting the rigor of this model checking approach, Bayesians have other, more analytical methods. \n",
    "\n",
    "### Model Check II: Bayes Factor\n",
    "Another modeling technique is to compute the Bayes factor. This is an analytical method that aims to compare two models with each other.\n",
    "\n",
    "The Bayes factor was typically a difficult metric to compute because it required integrating over the full joint probability distribution. In a low dimension space, integration is possible but once you begin to model in even modest dimensionality, integrating over the full joint posterior distribution becomes computationally costly and time-consuming.\n",
    "\n",
    "There is an alternative and analogous technique for calculating the Bayes factor. It involves taking your two models for comparison and combining them into a hierarchical model with a model parameter index ($\\tau$). This index will switch between the two models throughout the MCMC process depending on which model it finds more credible. As such, the trace of the model index tells us a lot about the credibility of model M1 over model M2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image('image/Bayes Factor DAG.png', width=540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \n",
    "    # Index to true model\n",
    "    prior_model_prob = 0.5\n",
    "    #tau = pm.DiscreteUniform('tau', lower=0, upper=1)\n",
    "    tau = pm.Bernoulli('tau', prior_model_prob)\n",
    "    \n",
    "    # Poisson parameters\n",
    "    mu_p = pm.Uniform('mu_p', 0, 60)\n",
    "\n",
    "    # Negative Binomial parameters\n",
    "    alpha = pm.Exponential('alpha', lam=0.2)\n",
    "    mu_nb = pm.Uniform('mu_nb', lower=0, upper=60)\n",
    "\n",
    "    y_like = pm.DensityDist('y_like',\n",
    "             lambda value: pm.math.switch(tau, \n",
    "                 pm.Poisson.dist(mu_p).logp(value),\n",
    "                 pm.NegativeBinomial.dist(mu_nb, alpha).logp(value)\n",
    "             ),\n",
    "             observed=messages['time_delay_seconds'].values)\n",
    "    \n",
    "    start = pm.find_MAP()\n",
    "    step1 = pm.Metropolis([mu_p, alpha, mu_nb])\n",
    "    step2 = pm.ElemwiseCategorical(vars=[tau], values=[0,1])\n",
    "    trace = pm.sample(200000, step=[step1, step2], start=start)\n",
    "\n",
    "_ = pm.traceplot(trace[burnin:], varnames=['tau'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the Bayes Factor for the above two models using the below formulation:\n",
    "\n",
    "$$Posterior Odds = Bayes Factor * Prior Odds$$\n",
    "\n",
    "$$\\frac{P(Data \\ | \\ M_{1})}{P(Data \\ | \\ M_{2})} = B.F. \\times \\frac{P(M_{1})}{P(M_{2})}$$\n",
    "\n",
    "In the above example, we didn't apply prior probability to either model, hence the Bayes Factor is simply the quotient of the model likelihoods. If you find that your MCMC sampler is not traversing between the two models, you can introduce prior probabilities that will help you get sufficient exposure to both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the Bayes factor\n",
    "prob_pois = trace[burnin:]['tau'].mean()\n",
    "prob_nb = 1 - prob_pois\n",
    "BF = (prob_nb/prob_pois)*(prior_model_prob/(1-prior_model_prob))\n",
    "print(\"Bayes Factor: %s\" % BF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayes Factor of >1 suggests that $M_1$ (Negative Binomial) is more strongly supported by the data than $M_2$ (Poisson). Jeffreys' scale of evidence for Bayes factors interprets a BF of 1.60 as there being weak evidence of $M_1$ over $M_2$ given the data. Combining the posterior predictive check and Bayes factor I will conclude that the Negative Binomial is a better model for the given data.\n",
    "\n",
    "| Bayes Factor                            | Interpretation                      |\n",
    "|-----------------------------------------|-------------------------------------|\n",
    "| BF($M_1, M_2$) < 1/10       | Strong evidence for $M_2$   |\n",
    "| 1/10 < BF($M_1, M_2$),< 1/3 | Moderate evidence for $M_2$ |\n",
    "| 1/3 < BF($M_1, M_2$) < 1    | Weak evidence for $M_2$     |\n",
    "| 1 < BF($M_1, M_2$) < 3      | Weak evidence for $M_1$     |\n",
    "| 3 < BF($M_1, M_2$) < 10     | Moderate evidence for $M_1$ |\n",
    "| BF($M_1, M_2$) > 10         | Strong evidence for $M_1$   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hierarchal modeling\n",
    "A key strength of Bayesian modeling is the easy and flexibility with which one can implement a hierarchical model. This section will implement and compare a pooled & partially pooled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import seaborn.apionly as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "colors = ['#348ABD', '#A60628', '#7A68A6', '#467821', '#D55E00', \n",
    "          '#CC79A7', '#56B4E9', '#009E73', '#F0E442', '#0072B2']\n",
    "\n",
    "messages = pd.read_csv('data/hangout_chat_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pooling\n",
    "Let's explore a different way of modeling the response time for my hangout conversations. My intuition would suggest that my tendency to reply quickly to a chat depends on who I'm talking to. I might be more likely to respond quickly to my girlfriend than to a distant friend. As such, I could decide to model each conversation independently, estimating parameters $\\mu_i$ and $\\alpha_i$ for each conversation $i$.\n",
    "\n",
    "One consideration we must make, is that some conversations have very few messages compared to others. As such, our estimates of response time for conversations with few messages will have a higher degree of uncertainty than conversations with a large number of messages. The below plot illustrates the discrepancy in sample size per conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = messages.groupby('prev_sender')['conversation_id'].size().plot(\n",
    "    kind='bar', figsize=(12,3), title='Number of messages sent per recipient', color=colors[0])\n",
    "_ = ax.set_xlabel('Previous Sender')\n",
    "_ = ax.set_ylabel('Number of messages')\n",
    "_ = plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each message j and each conversation i, we represent the model as:\n",
    "\n",
    "$$y_{ji} \\sim NegBinomial(\\mu_i, \\alpha_i)$$\n",
    "$$\\mu_i = Uniform(0, 100)$$\n",
    "$$\\alpha_i = Uniform(0, 100)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indiv_traces = {}\n",
    "\n",
    "# Convert categorical variables to integer\n",
    "le = preprocessing.LabelEncoder()\n",
    "participants_idx = le.fit_transform(messages['prev_sender'])\n",
    "participants = le.classes_\n",
    "n_participants = len(participants)\n",
    "\n",
    "for p in participants:\n",
    "    with pm.Model() as model:\n",
    "        alpha = pm.Uniform('alpha', lower=0, upper=100)\n",
    "        mu = pm.Uniform('mu', lower=0, upper=100)\n",
    "        \n",
    "        data = messages[messages['prev_sender']==p]['time_delay_seconds'].values\n",
    "        y_est = pm.NegativeBinomial('y_est', mu=mu, alpha=alpha, observed=data)\n",
    "\n",
    "        y_pred = pm.NegativeBinomial('y_pred', mu=mu, alpha=alpha)\n",
    "        \n",
    "        start = pm.find_MAP()\n",
    "        step = pm.Metropolis()\n",
    "        trace = pm.sample(20000, step, start=start, progressbar=True)\n",
    "        \n",
    "        indiv_traces[p] = trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2, figsize=(12, 6))\n",
    "axs = axs.ravel()\n",
    "y_left_max = 2\n",
    "y_right_max = 2000\n",
    "x_lim = 60\n",
    "ix = [3,4,6]\n",
    "\n",
    "for i, j, p in zip([0,1,2], [0,2,4], participants[ix]):\n",
    "    axs[j].set_title('Observed: %s' % p)\n",
    "    axs[j].hist(messages[messages['prev_sender']==p]['time_delay_seconds'].values, range=[0, x_lim], bins=x_lim, histtype='stepfilled')\n",
    "    axs[j].set_ylim([0, y_left_max])\n",
    "\n",
    "for i, j, p in zip([0,1,2], [1,3,5], participants[ix]):\n",
    "    axs[j].set_title('Posterior predictive distribution: %s' % p)\n",
    "    axs[j].hist(indiv_traces[p].get_values('y_pred'), range=[0, x_lim], bins=x_lim, histtype='stepfilled', color=colors[1])\n",
    "    axs[j].set_ylim([0, y_right_max])\n",
    "\n",
    "axs[4].set_xlabel('Response time (seconds)')\n",
    "axs[5].set_xlabel('Response time (seconds)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots show the observed data (left) and the posterior predictive distribution (right) for 3 example conversations we modeled. As you can see, the posterior predictive distribution can vary considerably across conversations. This could accurately reflect the characteristics of the conversation or it could be inaccurate due to small sample size.\n",
    "\n",
    "If we combine the posterior predictive distributions across these models, we would expect this to resemble the distribution of the overall dataset observed. Let's perform the posterior predictive check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_y_pred = np.concatenate([v.get_values('y_pred') for k, v in indiv_traces.items()])\n",
    "\n",
    "x_lim = 60\n",
    "y_pred = trace.get_values('y_pred')\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.add_subplot(211)\n",
    "\n",
    "fig.add_subplot(211)\n",
    "\n",
    "_ = plt.hist(combined_y_pred, range=[0, x_lim], bins=x_lim, histtype='stepfilled', color=colors[1])   \n",
    "_ = plt.xlim(1, x_lim)\n",
    "_ = plt.ylim(0, 20000)\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Posterior predictive distribution')\n",
    "\n",
    "fig.add_subplot(212)\n",
    "\n",
    "_ = plt.hist(messages['time_delay_seconds'].values, range=[0, x_lim], bins=x_lim, histtype='stepfilled')\n",
    "_ = plt.xlim(0, x_lim)\n",
    "_ = plt.xlabel('Response time in seconds')\n",
    "_ = plt.ylim(0, 20)\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Distribution of observed data')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the posterior predictive distribution resembles the distribution of the observed data. However, I'm concerned that some of the conversations have very little data and hence the estimates are likely to have high variance. One way to mitigate this risk to to share information across conversations - but still estimate $\\mu_i$ for each conversation. We call this partial pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial pooling\n",
    "Just like in the pooled model, a partially pooled model has paramater values estimated for each conversation i. However, parameters are connected together via hyperparameters. This reflects our belief that my `response_time`'s per conversation have similarities with one another via my own natural tendancy to respond quickly or slowly.\n",
    "\n",
    "$$y_{ji} \\sim NegBinomial(\\mu_i, \\alpha_i)$$\n",
    "\n",
    "Following on from the above example, we will estimate parameter values $(\\mu_i)$ and $(\\alpha_i)$ for a Poisson distribution. Rather than using a uniform prior, I will use a Gamma distribution for both $\\mu$ and $\\sigma$. This will enable me to introduce more prior knowledge into the model as I have certain expectations as to what vales $\\mu$ and $\\sigma$ will be.\n",
    "\n",
    "First, let's have a look at the Gamma distribution. As you can see below, it is very flexible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = [5,25,50]\n",
    "sd = [3,7,2]\n",
    "\n",
    "plt.figure(figsize=(11,3))\n",
    "_ = plt.title('Gamma distribution')\n",
    "\n",
    "with pm.Model() as model:\n",
    "    for i, (j, k) in enumerate(zip(mu, sd)):\n",
    "        samples = pm.Gamma('gamma_%s' % i, mu=j, sd=k).random(size=10**6)\n",
    "        plt.hist(samples, bins=100, range=(0,60), color=colors[i], alpha=1)\n",
    "\n",
    "_ = plt.legend(['$\\mu$ = %s, $\\sigma$ = %s' % (mu[a], sd[a]) for a in [0,1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partially pooled model can be formally described by:\n",
    "\n",
    "$$y_{ji} \\sim NegBinomial(\\mu_i, \\alpha_i)$$\n",
    "$$\\mu_i = Gamma(\\mu_\\mu, \\sigma_\\mu)$$\n",
    "$$\\alpha_i = Gamma(\\mu_\\alpha, \\sigma_\\alpha)$$\n",
    "$$\\mu_\\mu = Uniform(0, 60)$$\n",
    "$$\\sigma_\\mu = Uniform(0, 50)$$\n",
    "$$\\mu_\\alpha = Uniform(0, 10)$$\n",
    "$$\\sigma_\\alpha = Uniform(0, 50)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image('image/dag neg poisson gamma hyper.png', width=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    hyper_alpha_sd = pm.Uniform('hyper_alpha_sd', lower=0, upper=50)\n",
    "    hyper_alpha_mu = pm.Uniform('hyper_alpha_mu', lower=0, upper=10)\n",
    "    \n",
    "    hyper_mu_sd = pm.Uniform('hyper_mu_sd', lower=0, upper=50)\n",
    "    hyper_mu_mu = pm.Uniform('hyper_mu_mu', lower=0, upper=60)\n",
    "    \n",
    "    alpha = pm.Gamma('alpha', mu=hyper_alpha_mu, sd=hyper_alpha_sd, shape=n_participants)\n",
    "    mu = pm.Gamma('mu', mu=hyper_mu_mu, sd=hyper_mu_sd, shape=n_participants)\n",
    "    \n",
    "    y_est = pm.NegativeBinomial('y_est', \n",
    "                                mu=mu[participants_idx], \n",
    "                                alpha=alpha[participants_idx], \n",
    "                                observed=messages['time_delay_seconds'].values)\n",
    "    \n",
    "    y_pred = pm.NegativeBinomial('y_pred', \n",
    "                                 mu=mu[participants_idx], \n",
    "                                 alpha=alpha[participants_idx],\n",
    "                                 shape=messages['prev_sender'].shape)\n",
    "    \n",
    "    start = pm.find_MAP()\n",
    "    step = pm.Metropolis()\n",
    "    hierarchical_trace = pm.sample(200000, step, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = pm.traceplot(hierarchical_trace[120000:], \n",
    "                 varnames=['mu','alpha','hyper_mu_mu',\n",
    "                           'hyper_mu_sd','hyper_alpha_mu',\n",
    "                           'hyper_alpha_sd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see for the estimates of $\\mu$ and $\\alpha$ that we have multiple plots - one for each conversation i. The difference between the pooled and the partially pooled model is that the parameters of the partially pooled model ($\\mu_i$ and $\\alpha_i$) have a hyperparameter that is shared across all conversations i. This brings two benefits:\n",
    "1. Information is shared across conversations, so for conversations that have limited sample size, they \"borrow\" knowledge from other conversations during estimation to help reduce the variance of the estimate\n",
    "2. We get an estimate for each conversation and an overall estimate for all conversations\n",
    "\n",
    "Let's have a quick look at the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_lim = 60\n",
    "y_pred = hierarchical_trace.get_values('y_pred')[::1000].ravel()\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.add_subplot(211)\n",
    "\n",
    "fig.add_subplot(211)\n",
    "\n",
    "_ = plt.hist(y_pred, range=[0, x_lim], bins=x_lim, histtype='stepfilled', color=colors[1])   \n",
    "_ = plt.xlim(1, x_lim)\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Posterior predictive distribution')\n",
    "\n",
    "fig.add_subplot(212)\n",
    "\n",
    "_ = plt.hist(messages['time_delay_seconds'].values, range=[0, x_lim], bins=x_lim, histtype='stepfilled')\n",
    "_ = plt.xlabel('Response time in seconds')\n",
    "_ = plt.ylabel('Frequency')\n",
    "_ = plt.title('Distribution of observed data')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage effect: pooled vs hierarchical model\n",
    "As discussed, the partially pooled model shared a hyperparameter for both $\\mu$ and $\\alpha$. By sharing knowledge across conversations, it has the effect of shrinking the estimates closer together - particularly for conversations that have little data.\n",
    "\n",
    "This shrinkage effect is illustrated in the below plot. You can see how the $\\mu$ and $\\alpha$ parameters are drawn together by the effect of the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hier_mu = hierarchical_trace['mu'][500:].mean(axis=0)\n",
    "hier_alpha = hierarchical_trace['alpha'][500:].mean(axis=0)\n",
    "indv_mu = [indiv_traces[p]['mu'][500:].mean() for p in participants]\n",
    "indv_alpha = [indiv_traces[p]['alpha'][500:].mean() for p in participants]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, xlabel='mu', ylabel='alpha', \n",
    "                     title='Pooled vs. Partially Pooled Negative Binomial Model', \n",
    "                     xlim=(5, 45), ylim=(0, 10))\n",
    "\n",
    "ax.scatter(indv_mu, indv_alpha, c=colors[5], s=50, label = 'Pooled', zorder=3)\n",
    "ax.scatter(hier_mu, hier_alpha, c=colors[6], s=50, label = 'Partially Pooled', zorder=4)\n",
    "for i in range(len(indv_mu)):  \n",
    "    ax.arrow(indv_mu[i], indv_alpha[i], hier_mu[i] - indv_mu[i], hier_alpha[i] - indv_alpha[i], \n",
    "            fc=\"grey\", ec=\"grey\", length_includes_head=True, alpha=.5, head_width=0)\n",
    "\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking questions of the posterior\n",
    "Let's start to take advantage of one of the best aspects of Bayesian statistics - the posterior distribution. Unlike frequentist techniques, we get a full posterior distribution as opposed to a single point estimate. In essence, we have a basket full of credible parameter values. This enables us to ask some questions in a fairly natural and intuitive manner.\n",
    "\n",
    "#### What are the chances I'll respond to my friend in less than 10 seconds?\n",
    "To estimate this probability, we can look at the posterior predctive distribution for Timothy & Andrew's `response_time` and check how many of the samples are < 10 seconds. When I first heard of this technique, I thought I misunderstood because it seemed overly simplistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def participant_y_pred(person):\n",
    "    \"\"\"Return posterior predictive for person\"\"\"\n",
    "    ix = np.where(participants == person)[0][0]\n",
    "    return hierarchical_trace['y_pred'][100000:, ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Here are some samples from Timothy's posterior predictive distribution: \\n %s\" % participant_y_pred('Yonas'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def person_plotA(person_name):\n",
    "    ix_check = participant_y_pred(person_name) > 10\n",
    "    _ = plt.hist(participant_y_pred(person_name)[~ix_check], range=[0, x_lim], bins=x_lim, histtype='stepfilled', label='<10 seconds')\n",
    "    _ = plt.hist(participant_y_pred(person_name)[ix_check], range=[0, x_lim], bins=x_lim, histtype='stepfilled', label='>10 seconds')\n",
    "    _ = plt.title('Posterior predictive \\ndistribution for %s' % person_name)\n",
    "    _ = plt.xlabel('Response time')\n",
    "    _ = plt.ylabel('Frequency')\n",
    "    _ = plt.legend()\n",
    "\n",
    "def person_plotB(person_name):\n",
    "    x = np.linspace(1, 60, num=60)\n",
    "    num_samples = float(len(participant_y_pred(person_name)))\n",
    "    prob_lt_x = [100*sum(participant_y_pred(person_name) < i)/num_samples for i in x]\n",
    "    _ = plt.plot(x, prob_lt_x, color=colors[4])\n",
    "    _ = plt.fill_between(x, prob_lt_x, color=colors[4], alpha=0.3)\n",
    "    _ = plt.scatter(10, float(100*sum(participant_y_pred(person_name) < 10))/num_samples, s=180, c=colors[4])\n",
    "    _ = plt.title('Probability of responding \\nto %s before time (t)' % person_name)\n",
    "    _ = plt.xlabel('Response time (t)')\n",
    "    _ = plt.ylabel('Cumulative probability t')\n",
    "    _ = plt.ylim(ymin=0, ymax=100)\n",
    "    _ = plt.xlim(xmin=0, xmax=60)\n",
    "\n",
    "fig = plt.figure(figsize=(11,6))\n",
    "_ = fig.add_subplot(221)\n",
    "person_plotA('Anna')\n",
    "_ = fig.add_subplot(222)\n",
    "person_plotB('Anna')\n",
    "\n",
    "_ = fig.add_subplot(223)\n",
    "person_plotA('Yonas')\n",
    "_ = fig.add_subplot(224)\n",
    "person_plotB('Yonas')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find this methodology to be very intuitive and flexible. The plot above left separates the samples from the posterior predictive in terms of being greater than or less than 10 seconds. We can compute the probability by calculating the proportion of samples that are less than 10. The plot on the right simply computes this probability for each response time value from 0 to 60. So, it looks like Anna & Yonas have a 36% & 20% chance of being responded to in less than 10 seconds, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do my friends pair off against each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_persona_faster(persona, personb):\n",
    "    return np.float(sum(participant_y_pred(persona) < participant_y_pred(personb)))/len(participant_y_pred(persona))\n",
    "\n",
    "print(\"Probability that Tom is responded to faster than Andrew: {:.2%}\".format(prob_persona_faster('Anna', 'Yonas')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "ab_dist_df = pd.DataFrame(index=participants, columns=participants, dtype=np.float)\n",
    "\n",
    "# populate each cell in dataframe with persona_less_personb()\n",
    "for a, b in itertools.permutations(participants, 2):\n",
    "    ab_dist_df.ix[a, b] = prob_persona_faster(a, b)\n",
    "    \n",
    "# populate the diagonal\n",
    "for a in participants:\n",
    "    ab_dist_df.ix[a, a] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "cmap = plt.get_cmap(\"Spectral\")\n",
    "_ = sns.heatmap(ab_dist_df, square=True, cmap=cmap)\n",
    "_ = plt.title('Probability that Person A will be responded to faster than Person B')\n",
    "_ = plt.ylabel('Person A')\n",
    "_ = plt.xlabel('Person B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Bayesian Regression\n",
    "\n",
    "Previously, we addressed the question: \"is my chat response time effected by who I'm talking to?\". We have estimated model parameters for each individual I've had conversations with. But sometimes we want to understand the effect of more factors such as \"day of week,\" \"time of day,\" etc. We can use GLM (generalized linear models) to better understand the effects of these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import seaborn.apionly as sns\n",
    "import statsmodels.api as sm\n",
    "import theano.tensor as tt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "colors = ['#348ABD', '#A60628', '#7A68A6', '#467821', '#D55E00', \n",
    "          '#CC79A7', '#56B4E9', '#009E73', '#F0E442', '#0072B2']\n",
    "\n",
    "messages = pd.read_csv('data/hangout_chat_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression reminder\n",
    "\n",
    "When we have a response $y$ that is continuous from $-\\infty$ to $\\infty$, we can consider using a linear regression represented by: \n",
    "\n",
    "$$y \\sim \\mathcal{N}(\\mu, \\sigma)$$\n",
    "$$\\mu = \\beta_0 + \\beta_1 X_1 ... \\beta_n X_n$$\n",
    "\n",
    "We read this as: our response is normally distributed around $\\mu$ with a standard deviation of $\\sigma$. The value of $\\mu$ is described by a linear function of explanatory variables $X \\beta$ with a baseline intercept $\\beta_0$.\n",
    "\n",
    "### Link functions\n",
    "\n",
    "In the event you're not modeling a continuous response variable from $-\\infty$ to $\\infty$, you may need to use a link function to transform your response range. For a Poisson distribution, the canonical link function used is the log link. This can be formally described as:\n",
    "\n",
    "$$y \\sim Poi(\\mu)$$\n",
    "$$log(\\mu) = \\beta_0 + \\beta_1 X_1 ... \\beta_n X_n$$\n",
    "$$\\mu = e^{(\\beta_0 + \\beta_1 X_1 ... \\beta_n X_n)}$$\n",
    "\n",
    "This is considered to be a fixed effects model. The $\\beta$ coefficients are estimated across the entire population as opposed to estimating separate parameters for each person (like in the pooled and partially pooled model in Section 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed effects Poisson regression\n",
    "\n",
    "To construct a Poisson regression in PyMC3, we need to apply the log link function to $\\mu$.  The underlying data model in PyMC3 uses theano and hence we need to use the theano tensor method `theano.tensor.exp()` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = messages[['is_weekend','day_of_week','message_length','num_participants']].values\n",
    "_, num_X = X.shape\n",
    "\n",
    "with pm.Model() as model:       \n",
    "    intercept = pm.Normal('intercept', mu=0, sd=100)\n",
    "    beta_message_length = pm.Normal('beta_message_length', mu=0, sd=100)\n",
    "    beta_is_weekend = pm.Normal('beta_is_weekend', mu=0, sd=100)\n",
    "    beta_num_participants = pm.Normal('beta_num_participants', mu=0, sd=100)\n",
    "    \n",
    "    mu = tt.exp(intercept \n",
    "                + beta_message_length*messages.message_length \n",
    "                + beta_is_weekend*messages.is_weekend\n",
    "                + beta_num_participants*messages.num_participants)\n",
    "    \n",
    "    y_est = pm.Poisson('y_est', mu=mu, observed=messages['time_delay_seconds'].values)\n",
    "    \n",
    "    start = pm.find_MAP()\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(200000, step, start=start, progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above results, the baseline intercept $\\beta_0$ has an estimated value of between 2.5 and 2.9. So what does this mean?\n",
    "\n",
    "Unfortunately, interpreting the parameters of a Poisson regression is more involved than a simple linear regression (y = $\\beta$ x). In this linear regression, we could say for every unit increase in x, $\\hat{y}$ increases by $\\beta$. However, in the Poisson regression we need to consider the link function. The [following cross validated post](http://stats.stackexchange.com/questions/128926/how-to-interpret-parameter-estimates-in-poisson-glm-results) explains in great detail how we arrive at the below formulation.\n",
    "\n",
    "> For a Poisson model, given a unit change in $x$, the fitted $\\hat y$ changes by $\\hat y \\left( e^\\beta - 1 \\right)$\n",
    "\n",
    "The main takeaway from this is that the effect of changing x depends on the current value of y. Unlike the simple linear regression, a unit change in x does not cause a consistent change in y.\n",
    "\n",
    "###  Marginal and pairwise density plots\n",
    "\n",
    "The below plot shows the marginal densities (across the diagonal) and the pairwise densities (lower and upper panes). This plot is very useful for understanding how covariates interact with one another. In the above example, we can see that as the number of participants increases, the baseline intercept decreases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = sns.pairplot(pm.trace_to_dataframe(trace[20000:]), plot_kws={'alpha':.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed effects poisson regression\n",
    "\n",
    "We can make a small extension to the above model by including a random intercept parameter. This will allow us to estimate a baseline parameter value $\\beta_0$ for each person I communicate with. For all other parameters I will estimate a parameter across the entire population. For each person i and each message j, this is formally represented as:\n",
    "\n",
    "$$y_{ji} \\sim Poi(\\mu)$$\n",
    "$$\\mu = \\beta_{0_i} + \\beta_1 x_1 ... \\beta_n x_n$$\n",
    "\n",
    "By introducing this random effects parameter $\\beta_0$ for each person i, it allows the model to establish a different baseline for each person responded to - whilst estimating the  effects of the covariates on the response for the entire population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert categorical variables to integer\n",
    "le = preprocessing.LabelEncoder()\n",
    "participants_idx = le.fit_transform(messages['prev_sender'])\n",
    "participants = le.classes_\n",
    "n_participants = len(participants)\n",
    "\n",
    "with pm.Model() as model:\n",
    "\n",
    "    intercept = pm.Normal('intercept', mu=0, sd=100, shape=n_participants)\n",
    "    slope_message_length = pm.Normal('slope_message_length', mu=0, sd=100)\n",
    "    slope_is_weekend = pm.Normal('slope_is_weekend', mu=0, sd=100)\n",
    "    slope_num_participants = pm.Normal('slope_num_participants', mu=0, sd=100)\n",
    "    \n",
    "    mu = tt.exp(intercept[participants_idx] \n",
    "                + slope_message_length*messages.message_length \n",
    "                + slope_is_weekend*messages.is_weekend\n",
    "                + slope_num_participants*messages.num_participants)\n",
    "    \n",
    "    y_est = pm.Poisson('y_est', mu=mu, observed=messages['time_delay_seconds'].values)\n",
    "    \n",
    "    start = pm.find_MAP()\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(200000, step, start=start, progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the above results are interesting:\n",
    "- Each person has a different baseline response rate (as shown in the pooled and partially pooled model in Section 3)\n",
    "- Longer messages take marginally longer to respond to\n",
    "- You are more likely to get a slow response if you message me on the weekend\n",
    "- I tend to reply more quickly to conversations that have multiple people added to it (group  chat)\n",
    "\n",
    "And after accounting for the effect of each covariate on the response, the model estimates the below $\\beta_0$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(5, 6))\n",
    "_ = pm.forestplot(trace[20000:], varnames=['intercept'], ylabels=participants)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
