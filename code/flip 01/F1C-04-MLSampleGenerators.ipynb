{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP(01):  Advanced Data Science\n",
    "**(Module 02: Machine Learning)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Session 04 MLSampleGenerators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample generators\n",
    "## Generators for classification and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These generators produce a matrix of features and corresponding discrete targets.\n",
    "### Single label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.\n",
    "\n",
    "make_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem.\n",
    "\n",
    "make_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. produces Gaussian data with a spherical decision boundary for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n",
    "                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot several randomly generated 2D classification datasets.\n",
    "This example illustrates the :func:`datasets.make_classification`\n",
    ":func:`datasets.make_blobs` and :func:`datasets.make_gaussian_quantiles`\n",
    "functions.\n",
    "\n",
    "For ``make_classification``, three binary and two multi-class classification\n",
    "datasets are generated, with different numbers of informative features and\n",
    "clusters per class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.title(\"One informative feature, one cluster per class\", fontsize='small')\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,\n",
    "                             n_clusters_per_class=1)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.title(\"Two informative features, one cluster per class\", fontsize='small')\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.title(\"Two informative features, two clusters per class\",\n",
    "          fontsize='small')\n",
    "X2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)\n",
    "plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.title(\"Multi-class, two informative features, one cluster\",\n",
    "          fontsize='small')\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1, n_classes=3)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.title(\"Three blobs\", fontsize='small')\n",
    "X1, Y1 = make_blobs(n_features=2, centers=3)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "plt.subplot(326)\n",
    "plt.title(\"Gaussian divided into three quantiles\", fontsize='small')\n",
    "X1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include:\n",
    "\n",
    "    * Per-topic word distributions are independently drawn,where in reality all would be affected by a sparse base distribution, and would be correlated.\n",
    "    * For a document generated from multiple topics,all topics are weighted equally in generating its bag of words.\n",
    "    * Documents without labels words at random,rather than from a base distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates the `datasets.make_multilabel_classification` dataset\n",
    "generator. Each sample consists of counts of two features (up to 50 in\n",
    "total), which are differently distributed in each of two classes.\n",
    "\n",
    "Points are labeled as follows, where Y means the class is present:\n",
    "\n",
    "    =====  =====  =====  ======\n",
    "      1      2      3    Color\n",
    "    =====  =====  =====  ======\n",
    "      Y      N      N    Red\n",
    "      N      Y      N    Blue\n",
    "      N      N      Y    Yellow\n",
    "      Y      Y      N    Purple\n",
    "      Y      N      Y    Orange\n",
    "      Y      Y      N    Green\n",
    "      Y      Y      Y    Brown\n",
    "    =====  =====  =====  ======\n",
    "\n",
    "A star marks the expected sample for each class; its size reflects the\n",
    "probability of selecting that class label.\n",
    "\n",
    "The left and right examples highlight the ``n_labels`` parameter:\n",
    "more of the samples in the right plot have 2 or 3 labels.\n",
    "\n",
    "Note that this two-dimensional example is very degenerate:\n",
    "generally the number of features would be much greater than the\n",
    "\"document length\", while here we have much larger documents than vocabulary.\n",
    "Similarly, with ``n_classes > n_features``, it is much less likely that a\n",
    "feature distinguishes a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification as make_ml_clf\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "COLORS = np.array(['!',\n",
    "                   '#FF3333',  # red\n",
    "                   '#0198E1',  # blue\n",
    "                   '#BF5FFF',  # purple\n",
    "                   '#FCD116',  # yellow\n",
    "                   '#FF7216',  # orange\n",
    "                   '#4DBD33',  # green\n",
    "                   '#87421F'   # brown\n",
    "                   ])\n",
    "\n",
    "# Use same random seed for multiple calls to make_multilabel_classification to\n",
    "# ensure same distributions\n",
    "RANDOM_SEED = np.random.randint(2 ** 10)\n",
    "\n",
    "\n",
    "def plot_2d(ax, n_labels=1, n_classes=3, length=50):\n",
    "    X, Y, p_c, p_w_c = make_ml_clf(n_samples=150, n_features=2,\n",
    "                                   n_classes=n_classes, n_labels=n_labels,\n",
    "                                   length=length, allow_unlabeled=False,\n",
    "                                   return_distributions=True,\n",
    "                                   random_state=RANDOM_SEED)\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]\n",
    "                                                    ).sum(axis=1)),\n",
    "               marker='.')\n",
    "    ax.scatter(p_w_c[0] * length, p_w_c[1] * length,\n",
    "               marker='*', linewidth=.5, edgecolor='black',\n",
    "               s=20 + 1500 * p_c ** 2,\n",
    "               color=COLORS.take([1, 2, 4]))\n",
    "    ax.set_xlabel('Feature 0 count')\n",
    "    return p_c, p_w_c\n",
    "\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, sharex='row', sharey='row', figsize=(8, 4))\n",
    "plt.subplots_adjust(bottom=.15)\n",
    "\n",
    "p_c, p_w_c = plot_2d(ax1, n_labels=1)\n",
    "ax1.set_title('n_labels=1, length=50')\n",
    "ax1.set_ylabel('Feature 1 count')\n",
    "\n",
    "plot_2d(ax2, n_labels=3)\n",
    "ax2.set_title('n_labels=3, length=50')\n",
    "ax2.set_xlim(left=0, auto=True)\n",
    "ax2.set_ylim(bottom=0, auto=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('The data was generated from (random_state=%d):' % RANDOM_SEED)\n",
    "print('Class', 'P(C)', 'P(w0|C)', 'P(w1|C)', sep='\\t')\n",
    "for k, p, p_w in zip(['red', 'blue', 'yellow'], p_c, p_w_c.T):\n",
    "    print('%s\\t%0.2f\\t%0.2f\\t%0.2f' % (k, p, p_w[0], p_w[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
