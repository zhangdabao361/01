{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP(01):  Advanced Data Science\n",
    "**(Tools Module 04: TPOP)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Session 08 - TPOT for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyperparameter tuning is important\n",
    "\n",
    "One of the most tedious parts of machine learning is model hyperparameter tuning.\n",
    "\n",
    "Support vector machines require us to select the ideal kernel, the kernel’s parameters, and the penalty parameter C. Artificial neural networks require us to tune the number of hidden layers, number of hidden nodes, and many more hyperparameters. Even random forests require us to tune the number of trees in the ensemble at a minimum.\n",
    "\n",
    "All of these hyperparameters can have significant impacts on how well the model performs. For example, on the MNIST handwritten digit data set:\n",
    "\n",
    "<img src=\"figures/MNIST.png\" style=\"height:400px\">\n",
    "\n",
    "If we fit a random forest classifier with only 10 trees (scikit-learn’s default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist_data = pd.read_csv(r'C:/Users/lenovo/ansel/Notebook_05/data/mnist.csv.gz', sep='\\t', compression='gzip')\n",
    "\n",
    "cv_scores = cross_val_score(RandomForestClassifier(n_estimators=10, n_jobs=-1),\n",
    "                            X=mnist_data.drop('class', axis=1).values,\n",
    "                            y=mnist_data.loc[:, 'class'].values,\n",
    "                            cv=10)\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest achieves an average of 94.7% cross-validation accuracy on MNIST. However, what if we tuned that hyperparameter a little bit and provided the random forest with 100 trees instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv_scores = cross_val_score(RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "                            X=mnist_data.drop('class', axis=1).values,\n",
    "                            y=mnist_data.loc[:, 'class'].values,\n",
    "                            cv=10)\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such a minor change, we improved the random forest’s average cross-validation accuracy from 94.7% to 96.9%. This small improvement in accuracy can translate into millions of additional digits classified correctly if we’re applying this model on the scale of, say, processing addresses for the U.S. Postal Service.\n",
    "\n",
    "**Never use the defaults for your model.** Hyperparameter tuning is vitally important for every machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection is important\n",
    "\n",
    "We all love to think that our favorite model will perform well on every machine learning problem, but different models are better suited for different tasks.\n",
    "\n",
    "For example, if we’re working on a signal processing problem where we need to classify whether there’s a “hill” or “valley” in the time series:\n",
    "\n",
    "<img src='figures/time-series.png' style='height:500px' >\n",
    "\n",
    "And we apply a “tuned” random forest to the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "hill_valley_data = pd.read_csv(r'C:/Users/lenovo/ansel/Notebook_05/data/Hill_Valley_without_noise.csv.gz', sep='\\t', compression='gzip')\n",
    "\n",
    "cv_scores = cross_val_score(RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "                            X=hill_valley_data.drop('class', axis=1).values,\n",
    "                            y=hill_valley_data.loc[:, 'class'].values,\n",
    "                            cv=10)\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we’re going to find that the random forest isn’t well-suited for signal processing tasks like this one when it achieves a disappointing average of 61.8% cross-validation accuracy.\n",
    "\n",
    "What if we tried a different model, for example a logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv_scores = cross_val_score(LogisticRegression(),\n",
    "                            X=hill_valley_data.drop('class', axis=1).values,\n",
    "                            y=hill_valley_data.loc[:, 'class'].values,\n",
    "                            cv=10)\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll find that a logistic regression is well-suited for this signal processing task—in fact, it easily achieves near-100% cross-validation accuracy without any hyperparameter tuning at all.\n",
    "\n",
    "**Always try out many different machine learning models** for every machine learning task that you work on. Trying out—and tuning—different machine learning models is another tedious yet vitally important step of machine learning pipeline design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing is important\n",
    "\n",
    "As we’ve seen in the previous two examples, machine learning model performance is also affected by how the features are represented. Feature preprocessing is a step in machine learning pipelines where we reshape the features in a manner that makes the data set easier for models to classify.\n",
    "\n",
    "For example, if we’re working on a harder version of the “hill” vs. “valley” signal processing problem with noise:\n",
    "\n",
    "<img src='figures/time-series-noise.png' style='height:500px'>\n",
    "\n",
    "And we apply a “tuned” random forest to the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "hill_valley_noisy_data = pd.read_csv(r'C:/Users/lenovo/ansel/Notebook_05/data/Hill_Valley_with_noise.csv.gz', sep='\\t', compression='gzip')\n",
    "\n",
    "cv_scores = cross_val_score(RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "                            X=hill_valley_noisy_data.drop('class', axis=1).values,\n",
    "                            y=hill_valley_noisy_data.loc[:, 'class'].values,\n",
    "                            cv=10)\n",
    "\n",
    "print(cv_scores)\n",
    "\n",
    "\n",
    "print(np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll again find that the “tuned” random forest averages a disappointing 57.8% cross-validation accuracy.\n",
    "\n",
    "However, if we preprocess the features—denoising them via Principal Component Analysis (PCA), for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv_scores = cross_val_score(make_pipeline(PCA(n_components=10),\n",
    "                                          RandomForestClassifier(n_estimators=100,\n",
    "                                                                 n_jobs=-1)),\n",
    "                            X=hill_valley_noisy_data.drop('class', axis=1).values,\n",
    "                            y=hill_valley_noisy_data.loc[:, 'class'].values,\n",
    "                            cv=10)\n",
    "\n",
    "print(cv_scores)\n",
    "\n",
    "\n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll find that the random forest now achieves an average of 94% cross-validation accuracy by applying a simple feature preprocessing step.\n",
    "\n",
    "**Always explore numerous feature representations for your data.** Machines learn differently from humans, and a feature representation that makes sense to us may not make sense to the machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating data science with TPOT\n",
    "\n",
    "To summarize what we’ve learned so far about effective machine learning system design, we should:\n",
    "\n",
    "* Always tune the hyperparameters for our models\n",
    "* Always try out many different models\n",
    "* Always explore numerous feature representations for our data\n",
    "\n",
    "We must also consider the following:\n",
    "\n",
    "* There are thousands of possible hyperparameter configurations for every model\n",
    "* There are dozens of popular machine learning models\n",
    "* There are dozens of popular feature preprocessing methods\n",
    "\n",
    "This is why it can be so tedious to design effective machine learning systems. This is also why my collaborators and I created [TPOT](https://github.com/EpistasisLab/tpot), an open source Python tool that intelligently automates the entire process.\n",
    "\n",
    "If your data set is [compatible with scikit-learn](https://github.com/amueller/scipy_2015_sklearn_tutorial/blob/master/notebooks/01.3%20Data%20Representation%20for%20Machine%20Learning.ipynb), then TPOT will automatically optimize a series of feature preprocessors and models that maximize the cross-validation accuracy on the data set. For example, if we want TPOT to solve the noisy “hill” vs. “valley” classification problem:\n",
    "\n",
    "(Before running the code below, make sure to [install TPOT first](https://epistasislab.github.io/tpot/installing/).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "hill_valley_noisy_data = pd.read_csv(r'C:/Users/lenovo/ansel/Notebook_05/data/Hill_Valley_with_noise.csv.gz', sep='\\t', compression='gzip')\n",
    "\n",
    "X = hill_valley_noisy_data.drop('class', axis=1).values\n",
    "y = hill_valley_noisy_data.loc[:, 'class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.75,\n",
    "                                                    test_size=0.25)\n",
    "\n",
    "my_tpot = TPOTClassifier(generations=10)\n",
    "my_tpot.fit(X_train, y_train)\n",
    "\n",
    "print(my_tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the machine you’re running it on, 10 TPOT generations should take about 5 minutes to complete. During this time, you’re free to browse Hacker News, refill your cup of coffee, or admire the beautiful weather outside. In the meantime, TPOT will handle all of the work for you.\n",
    "\n",
    "After 5 minutes of optimization, TPOT will discover a pipeline that achieves 96% cross-validation accuracy on the noisy “hill” vs. “valley” problem—better than the hand-designed pipeline we created above!\n",
    "\n",
    "If we want to see what pipeline TPOT created, TPOT can export the corresponding scikit-learn code for us with the export() command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tpot.export('exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)\n",
    "training_features, testing_features, training_classes, testing_classes = \\\n",
    "    train_test_split(features, tpot_data['class'], random_state=42)\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    Normalizer(norm=\"l2\"),\n",
    "    GradientBoostingClassifier(learning_rate=0.97, max_features=0.97, n_estimators=500)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_classes)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and shows us that a tuned gradient tree boosting classifier is probably the best model for this problem once the data has been normalized.\n",
    "\n",
    "We’ve designed TPOT to be an end-to-end automated machine learning system, which can act as a drop-in replacement for any scikit-learn model that you’re currently using in your workflow.\n",
    "\n",
    "If TPOT sounds like the tool you’ve been looking for, here’s a few links that you may find useful:\n",
    "\n",
    "* [TPOT repository on GitHub](https://github.com/EpistasisLab/tpot)\n",
    "* [TPOT documentation](https://epistasislab.github.io/tpot/)\n",
    "* [TPOT installation instructions](https://epistasislab.github.io/tpot/installing/)\n",
    "* [TPOT example code](https://epistasislab.github.io/tpot/examples/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
